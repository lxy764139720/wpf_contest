{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "         TurbID  Day Tmstamp   Wspd   Wdir   Etmp   Itmp    Ndir   Pab1  \\\n0             1    1   00:00  12.23  -0.83  29.08  41.90  -23.73   1.07   \n1             1    1   00:10  11.58  -3.32  29.01  42.01  -23.70   1.06   \n2             1    1   00:20  11.21  -1.38  29.17  42.24  -28.84   1.04   \n3             1    1   00:30  10.84   0.06  29.46  42.43  -31.39   1.03   \n4             1    1   00:40  11.03   2.03  29.82  42.77  -31.39   1.03   \n...         ...  ...     ...    ...    ...    ...    ...     ...    ...   \n3550459     134  184   23:10   2.36 -74.19   7.30  11.70  238.59  90.39   \n3550460     134  184   23:20   1.72 -67.92   7.21  11.70  238.59  90.39   \n3550461     134  184   23:30   1.46 -59.15   7.10  11.70  238.59  90.39   \n3550462     134  184   23:40   1.31 -64.11   7.10  11.70  238.59  90.39   \n3550463     134  184   23:50   1.23 -72.49   7.10  11.80  238.59  90.39   \n\n          Pab2   Pab3   Prtv     Patv  \n0         1.07   1.07  -0.21  1549.53  \n1         1.06   1.06  -0.25  1549.71  \n2         1.04   1.04  -0.25  1534.77  \n3         1.03   1.03  -0.25  1508.20  \n4         1.03   1.03 -66.01  1517.76  \n...        ...    ...    ...      ...  \n3550459  90.37  90.36  -0.30    -0.30  \n3550460  90.37  90.36  -0.30    -0.30  \n3550461  90.37  90.36  -0.30    -0.30  \n3550462  90.37  90.36  -0.30    -0.30  \n3550463  90.37  90.36  -0.30    -0.30  \n\n[3505711 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TurbID</th>\n      <th>Day</th>\n      <th>Tmstamp</th>\n      <th>Wspd</th>\n      <th>Wdir</th>\n      <th>Etmp</th>\n      <th>Itmp</th>\n      <th>Ndir</th>\n      <th>Pab1</th>\n      <th>Pab2</th>\n      <th>Pab3</th>\n      <th>Prtv</th>\n      <th>Patv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>00:00</td>\n      <td>12.23</td>\n      <td>-0.83</td>\n      <td>29.08</td>\n      <td>41.90</td>\n      <td>-23.73</td>\n      <td>1.07</td>\n      <td>1.07</td>\n      <td>1.07</td>\n      <td>-0.21</td>\n      <td>1549.53</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>00:10</td>\n      <td>11.58</td>\n      <td>-3.32</td>\n      <td>29.01</td>\n      <td>42.01</td>\n      <td>-23.70</td>\n      <td>1.06</td>\n      <td>1.06</td>\n      <td>1.06</td>\n      <td>-0.25</td>\n      <td>1549.71</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>00:20</td>\n      <td>11.21</td>\n      <td>-1.38</td>\n      <td>29.17</td>\n      <td>42.24</td>\n      <td>-28.84</td>\n      <td>1.04</td>\n      <td>1.04</td>\n      <td>1.04</td>\n      <td>-0.25</td>\n      <td>1534.77</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>00:30</td>\n      <td>10.84</td>\n      <td>0.06</td>\n      <td>29.46</td>\n      <td>42.43</td>\n      <td>-31.39</td>\n      <td>1.03</td>\n      <td>1.03</td>\n      <td>1.03</td>\n      <td>-0.25</td>\n      <td>1508.20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>00:40</td>\n      <td>11.03</td>\n      <td>2.03</td>\n      <td>29.82</td>\n      <td>42.77</td>\n      <td>-31.39</td>\n      <td>1.03</td>\n      <td>1.03</td>\n      <td>1.03</td>\n      <td>-66.01</td>\n      <td>1517.76</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3550459</th>\n      <td>134</td>\n      <td>184</td>\n      <td>23:10</td>\n      <td>2.36</td>\n      <td>-74.19</td>\n      <td>7.30</td>\n      <td>11.70</td>\n      <td>238.59</td>\n      <td>90.39</td>\n      <td>90.37</td>\n      <td>90.36</td>\n      <td>-0.30</td>\n      <td>-0.30</td>\n    </tr>\n    <tr>\n      <th>3550460</th>\n      <td>134</td>\n      <td>184</td>\n      <td>23:20</td>\n      <td>1.72</td>\n      <td>-67.92</td>\n      <td>7.21</td>\n      <td>11.70</td>\n      <td>238.59</td>\n      <td>90.39</td>\n      <td>90.37</td>\n      <td>90.36</td>\n      <td>-0.30</td>\n      <td>-0.30</td>\n    </tr>\n    <tr>\n      <th>3550461</th>\n      <td>134</td>\n      <td>184</td>\n      <td>23:30</td>\n      <td>1.46</td>\n      <td>-59.15</td>\n      <td>7.10</td>\n      <td>11.70</td>\n      <td>238.59</td>\n      <td>90.39</td>\n      <td>90.37</td>\n      <td>90.36</td>\n      <td>-0.30</td>\n      <td>-0.30</td>\n    </tr>\n    <tr>\n      <th>3550462</th>\n      <td>134</td>\n      <td>184</td>\n      <td>23:40</td>\n      <td>1.31</td>\n      <td>-64.11</td>\n      <td>7.10</td>\n      <td>11.70</td>\n      <td>238.59</td>\n      <td>90.39</td>\n      <td>90.37</td>\n      <td>90.36</td>\n      <td>-0.30</td>\n      <td>-0.30</td>\n    </tr>\n    <tr>\n      <th>3550463</th>\n      <td>134</td>\n      <td>184</td>\n      <td>23:50</td>\n      <td>1.23</td>\n      <td>-72.49</td>\n      <td>7.10</td>\n      <td>11.80</td>\n      <td>238.59</td>\n      <td>90.39</td>\n      <td>90.37</td>\n      <td>90.36</td>\n      <td>-0.30</td>\n      <td>-0.30</td>\n    </tr>\n  </tbody>\n</table>\n<p>3505711 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../data/'\n",
    "filename1 = 'sdwpf_baidukddcup2022_full.CSV'\n",
    "df = pd.read_csv(data_path + filename1)\n",
    "df.dropna(axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "i_tmp = df['Itmp'].values\n",
    "mean = np.nanmean(i_tmp)\n",
    "std = np.nanstd(i_tmp)\n",
    "threshold_low = mean - 3 * std\n",
    "threshold_high = mean + 3 * std\n",
    "i_tmp[i_tmp > threshold_high] = np.nan\n",
    "i_tmp[i_tmp < threshold_low] = np.nan\n",
    "mean = np.nanmean(i_tmp)\n",
    "std = np.nanstd(i_tmp)\n",
    "threshold_low = mean - 3 * std\n",
    "threshold_high = mean + 3 * std"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "e_tmp = df['Etmp'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "mean = np.nanmean(e_tmp)\n",
    "std = np.nanstd(e_tmp)\n",
    "threshold_low = mean - 3 * std\n",
    "threshold_high = mean + 3 * std\n",
    "#\n",
    "# valid_index = 288\n",
    "# normal = (e_tmp > threshold_low) & (e_tmp < threshold_high) & ~np.isnan(e_tmp)\n",
    "# print(normal[valid_index - 288: valid_index].all())\n",
    "# while not normal[valid_index - 288: valid_index].all():\n",
    "#     exist = np.argwhere(normal[valid_index + 1:] == True)\n",
    "#     valid_index = exist[0][0] + valid_index + 1\n",
    "#     print(exist[0][0] + valid_index + 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "e_tmp[e_tmp > threshold_high] = np.nan\n",
    "e_tmp[e_tmp < threshold_low] = np.nan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "mean = np.nanmean(e_tmp)\n",
    "std = np.nanstd(e_tmp)\n",
    "threshold_low = mean - 3 * std\n",
    "threshold_high = mean + 3 * std"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(20, 6), dpi=100)\n",
    "# x_major_locator = MultipleLocator(1)\n",
    "# ax = fig.gca()\n",
    "# ax.xaxis.set_major_locator(x_major_locator)\n",
    "# x_axis = np.array(range(len(e_tmp))) / 144\n",
    "# th_low_point = np.empty(len(e_tmp))\n",
    "# th_low_point.fill(threshold_low)\n",
    "# th_high_point = np.empty(len(e_tmp))\n",
    "# th_high_point.fill(threshold_high)\n",
    "# plt.plot(x_axis[:2000], e_tmp[:2000])\n",
    "# plt.plot(x_axis[:2000], th_low_point[:2000], color='black')\n",
    "# plt.plot(x_axis[:2000], th_high_point[:2000], color='black')\n",
    "# plt.legend()\n",
    "# fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# normal = (e_tmp > threshold_low) & (e_tmp < threshold_high) & ~np.isnan(e_tmp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# %%time\n",
    "# valid_index = 1481\n",
    "# print(normal[valid_index - 288: valid_index].all())\n",
    "# if normal[valid_index - 288: valid_index].all():\n",
    "#     print(valid_index)\n",
    "# while not normal[valid_index - 288: valid_index].all():\n",
    "#     valid_index += 1\n",
    "#     print(valid_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from notebooks.tmp_dataset import TMPDataset\n",
    "from notebooks.tmp_train import train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test X:  torch.Size([32, 288, 1])\n",
      "Shape of test y:  torch.Size([32, 1]) torch.float32\n",
      "===================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Kernel Shape              Output Shape\n",
      "===================================================================================================================\n",
      "LSTMModel                                --                        --                        --\n",
      "├─LSTM: 1-1                              [32, 288, 1]              --                        [32, 288, 32]\n",
      "│    └─weight_ih_l0                                                [64, 1]\n",
      "│    └─weight_hh_l0                                                [64, 16]\n",
      "│    └─bias_ih_l0                                                  [64]\n",
      "│    └─bias_hh_l0                                                  [64]\n",
      "│    └─weight_ih_l0_reverse                                        [64, 1]\n",
      "│    └─weight_hh_l0_reverse                                        [64, 16]\n",
      "│    └─bias_ih_l0_reverse                                          [64]\n",
      "│    └─bias_hh_l0_reverse                                          [64]\n",
      "│    └─weight_ih_l1                                                [64, 32]\n",
      "│    └─weight_hh_l1                                                [64, 16]\n",
      "│    └─bias_ih_l1                                                  [64]\n",
      "│    └─bias_hh_l1                                                  [64]\n",
      "│    └─weight_ih_l1_reverse                                        [64, 32]\n",
      "│    └─weight_hh_l1_reverse                                        [64, 16]\n",
      "│    └─bias_ih_l1_reverse                                          [64]\n",
      "│    └─bias_hh_l1_reverse                                          [64]\n",
      "├─Dropout: 1-2                           [32, 32]                  --                        [32, 32]\n",
      "├─Linear: 1-3                            [32, 32]                  [32, 1]                   [32, 1]\n",
      "│    └─weight                                                      [32, 1]\n",
      "│    └─bias                                                        [1]\n",
      "├─ReLU: 1-4                              [32, 1]                   --                        [32, 1]\n",
      "===================================================================================================================\n",
      "Total params: 8,865\n",
      "Trainable params: 8,865\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 81.40\n",
      "===================================================================================================================\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 2.36\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 2.43\n",
      "===================================================================================================================\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 32093.447266  [     0/ 35504]\n",
      "loss: 27574.580078  [    32/ 35504]\n",
      "loss: 27463.156250  [    64/ 35504]\n",
      "loss: 28454.537109  [    96/ 35504]\n",
      "loss: 24345.314453  [   128/ 35504]\n",
      "loss: 23795.886719  [   160/ 35504]\n",
      "loss: 24301.542969  [   192/ 35504]\n",
      "loss: 24761.726562  [   224/ 35504]\n",
      "loss: 26153.898438  [   256/ 35504]\n",
      "loss: 30622.417969  [   288/ 35504]\n",
      "loss: 25403.326172  [   320/ 35504]\n",
      "loss: 21817.914062  [   352/ 35504]\n",
      "loss: 26536.707031  [   384/ 35504]\n",
      "loss: 24298.746094  [   416/ 35504]\n",
      "loss: 26552.410156  [   448/ 35504]\n",
      "loss: 24435.437500  [   480/ 35504]\n",
      "loss: 23536.851562  [   512/ 35504]\n",
      "loss: 25873.511719  [   544/ 35504]\n",
      "loss: 27565.699219  [   576/ 35504]\n",
      "loss: 27199.226562  [   608/ 35504]\n",
      "loss: 23425.125000  [   640/ 35504]\n",
      "loss: 25925.519531  [   672/ 35504]\n",
      "loss: 27578.824219  [   704/ 35504]\n",
      "loss: 25906.068359  [   736/ 35504]\n",
      "loss: 22694.093750  [   768/ 35504]\n",
      "loss: 24913.037109  [   800/ 35504]\n",
      "loss: 25060.437500  [   832/ 35504]\n",
      "loss: 23524.125000  [   864/ 35504]\n",
      "loss: 26496.875000  [   896/ 35504]\n",
      "loss: 23915.945312  [   928/ 35504]\n",
      "loss: 19626.320312  [   960/ 35504]\n",
      "loss: 25798.568359  [   992/ 35504]\n",
      "loss: 25566.640625  [  1024/ 35504]\n",
      "loss: 30756.304688  [  1056/ 35504]\n",
      "loss: 23944.730469  [  1088/ 35504]\n",
      "loss: 27319.445312  [  1120/ 35504]\n",
      "loss: 23613.765625  [  1152/ 35504]\n",
      "loss: 26523.972656  [  1184/ 35504]\n",
      "loss: 20683.080078  [  1216/ 35504]\n",
      "loss: 19554.951172  [  1248/ 35504]\n",
      "loss: 26912.964844  [  1280/ 35504]\n",
      "loss: 23104.603516  [  1312/ 35504]\n",
      "loss: 26706.869141  [  1344/ 35504]\n",
      "loss: 25281.560547  [  1376/ 35504]\n",
      "loss: 25239.062500  [  1408/ 35504]\n",
      "loss: 22814.835938  [  1440/ 35504]\n",
      "loss: 25685.246094  [  1472/ 35504]\n",
      "loss: 27164.568359  [  1504/ 35504]\n",
      "loss: 24340.521484  [  1536/ 35504]\n",
      "loss: 22735.457031  [  1568/ 35504]\n",
      "loss: 25526.138672  [  1600/ 35504]\n",
      "loss: 22625.101562  [  1632/ 35504]\n",
      "loss: 22180.982422  [  1664/ 35504]\n",
      "loss: 23683.132812  [  1696/ 35504]\n",
      "loss: 21232.910156  [  1728/ 35504]\n",
      "loss: 25018.953125  [  1760/ 35504]\n",
      "loss: 21374.000000  [  1792/ 35504]\n",
      "loss: 22145.675781  [  1824/ 35504]\n",
      "loss: 26077.359375  [  1856/ 35504]\n",
      "loss: 24992.078125  [  1888/ 35504]\n",
      "loss: 23063.812500  [  1920/ 35504]\n",
      "loss: 17584.496094  [  1952/ 35504]\n",
      "loss: 21863.853516  [  1984/ 35504]\n",
      "loss: 20537.263672  [  2016/ 35504]\n",
      "loss: 21663.751953  [  2048/ 35504]\n",
      "loss: 26431.058594  [  2080/ 35504]\n",
      "loss: 22775.439453  [  2112/ 35504]\n",
      "loss: 21591.867188  [  2144/ 35504]\n",
      "loss: 21266.326172  [  2176/ 35504]\n",
      "loss: 21481.525391  [  2208/ 35504]\n",
      "loss: 21214.388672  [  2240/ 35504]\n",
      "loss: 21478.851562  [  2272/ 35504]\n",
      "loss: 23974.214844  [  2304/ 35504]\n",
      "loss: 21119.681641  [  2336/ 35504]\n",
      "loss: 20209.240234  [  2368/ 35504]\n",
      "loss: 24809.916016  [  2400/ 35504]\n",
      "loss: 19130.255859  [  2432/ 35504]\n",
      "loss: 20202.539062  [  2464/ 35504]\n",
      "loss: 19612.189453  [  2496/ 35504]\n",
      "loss: 19593.179688  [  2528/ 35504]\n",
      "loss: 20913.236328  [  2560/ 35504]\n",
      "loss: 22452.140625  [  2592/ 35504]\n",
      "loss: 20569.826172  [  2624/ 35504]\n",
      "loss: 21925.904297  [  2656/ 35504]\n",
      "loss: 18200.267578  [  2688/ 35504]\n",
      "loss: 16277.741211  [  2720/ 35504]\n",
      "loss: 20129.808594  [  2752/ 35504]\n",
      "loss: 18337.658203  [  2784/ 35504]\n",
      "loss: 17933.539062  [  2816/ 35504]\n",
      "loss: 19776.791016  [  2848/ 35504]\n",
      "loss: 17327.638672  [  2880/ 35504]\n",
      "loss: 23601.542969  [  2912/ 35504]\n",
      "loss: 19610.246094  [  2944/ 35504]\n",
      "loss: 18732.164062  [  2976/ 35504]\n",
      "loss: 17865.871094  [  3008/ 35504]\n",
      "loss: 22840.832031  [  3040/ 35504]\n",
      "loss: 14253.739258  [  3072/ 35504]\n",
      "loss: 19795.394531  [  3104/ 35504]\n",
      "loss: 17901.658203  [  3136/ 35504]\n",
      "loss: 16017.785156  [  3168/ 35504]\n",
      "loss: 19047.373047  [  3200/ 35504]\n",
      "loss: 17302.419922  [  3232/ 35504]\n",
      "loss: 18536.789062  [  3264/ 35504]\n",
      "loss: 19994.263672  [  3296/ 35504]\n",
      "loss: 21691.087891  [  3328/ 35504]\n",
      "loss: 15810.600586  [  3360/ 35504]\n",
      "loss: 19047.673828  [  3392/ 35504]\n",
      "loss: 16144.423828  [  3424/ 35504]\n",
      "loss: 19097.310547  [  3456/ 35504]\n",
      "loss: 18770.558594  [  3488/ 35504]\n",
      "loss: 17836.660156  [  3520/ 35504]\n",
      "loss: 17499.777344  [  3552/ 35504]\n",
      "loss: 17658.503906  [  3584/ 35504]\n",
      "loss: 15634.347656  [  3616/ 35504]\n",
      "loss: 18573.287109  [  3648/ 35504]\n",
      "loss: 17683.326172  [  3680/ 35504]\n",
      "loss: 14972.504883  [  3712/ 35504]\n",
      "loss: 17156.419922  [  3744/ 35504]\n",
      "loss: 16612.960938  [  3776/ 35504]\n",
      "loss: 15262.542969  [  3808/ 35504]\n",
      "loss: 16611.890625  [  3840/ 35504]\n",
      "loss: 18763.228516  [  3872/ 35504]\n",
      "loss: 18130.888672  [  3904/ 35504]\n",
      "loss: 15434.269531  [  3936/ 35504]\n",
      "loss: 15918.988281  [  3968/ 35504]\n",
      "loss: 18060.281250  [  4000/ 35504]\n",
      "loss: 17191.890625  [  4032/ 35504]\n",
      "loss: 19550.906250  [  4064/ 35504]\n",
      "loss: 14358.827148  [  4096/ 35504]\n",
      "loss: 17029.078125  [  4128/ 35504]\n",
      "loss: 14675.635742  [  4160/ 35504]\n",
      "loss: 13805.709961  [  4192/ 35504]\n",
      "loss: 16780.222656  [  4224/ 35504]\n",
      "loss: 19252.787109  [  4256/ 35504]\n",
      "loss: 14786.064453  [  4288/ 35504]\n",
      "loss: 15369.449219  [  4320/ 35504]\n",
      "loss: 12864.052734  [  4352/ 35504]\n",
      "loss: 15973.489258  [  4384/ 35504]\n",
      "loss: 15069.510742  [  4416/ 35504]\n",
      "loss: 19794.505859  [  4448/ 35504]\n",
      "loss: 14720.064453  [  4480/ 35504]\n",
      "loss: 15848.877930  [  4512/ 35504]\n",
      "loss: 15283.666016  [  4544/ 35504]\n",
      "loss: 17025.562500  [  4576/ 35504]\n",
      "loss: 15091.060547  [  4608/ 35504]\n",
      "loss: 15581.112305  [  4640/ 35504]\n",
      "loss: 17328.585938  [  4672/ 35504]\n",
      "loss: 15382.326172  [  4704/ 35504]\n",
      "loss: 16690.166016  [  4736/ 35504]\n",
      "loss: 16701.781250  [  4768/ 35504]\n",
      "loss: 15626.114258  [  4800/ 35504]\n",
      "loss: 14676.230469  [  4832/ 35504]\n",
      "loss: 13312.714844  [  4864/ 35504]\n",
      "loss: 14022.950195  [  4896/ 35504]\n",
      "loss: 19220.830078  [  4928/ 35504]\n",
      "loss: 13814.345703  [  4960/ 35504]\n",
      "loss: 16383.779297  [  4992/ 35504]\n",
      "loss: 12839.495117  [  5024/ 35504]\n",
      "loss: 12031.105469  [  5056/ 35504]\n",
      "loss: 17825.675781  [  5088/ 35504]\n",
      "loss: 15026.018555  [  5120/ 35504]\n",
      "loss: 12113.195312  [  5152/ 35504]\n",
      "loss: 13085.350586  [  5184/ 35504]\n",
      "loss: 14117.569336  [  5216/ 35504]\n",
      "loss: 12189.357422  [  5248/ 35504]\n",
      "loss: 15014.543945  [  5280/ 35504]\n",
      "loss: 19625.359375  [  5312/ 35504]\n",
      "loss: 12775.717773  [  5344/ 35504]\n",
      "loss: 13487.761719  [  5376/ 35504]\n",
      "loss: 13667.927734  [  5408/ 35504]\n",
      "loss: 15373.380859  [  5440/ 35504]\n",
      "loss: 13720.404297  [  5472/ 35504]\n",
      "loss: 15939.883789  [  5504/ 35504]\n",
      "loss: 14654.900391  [  5536/ 35504]\n",
      "loss: 14709.881836  [  5568/ 35504]\n",
      "loss: 13973.327148  [  5600/ 35504]\n",
      "loss: 14693.095703  [  5632/ 35504]\n",
      "loss: 12286.845703  [  5664/ 35504]\n",
      "loss: 14597.724609  [  5696/ 35504]\n",
      "loss: 12001.675781  [  5728/ 35504]\n",
      "loss: 12510.753906  [  5760/ 35504]\n",
      "loss: 12892.827148  [  5792/ 35504]\n",
      "loss: 11221.247070  [  5824/ 35504]\n",
      "loss: 12880.937500  [  5856/ 35504]\n",
      "loss: 13776.171875  [  5888/ 35504]\n",
      "loss: 11154.304688  [  5920/ 35504]\n",
      "loss: 13042.040039  [  5952/ 35504]\n",
      "loss: 10525.513672  [  5984/ 35504]\n",
      "loss: 11181.347656  [  6016/ 35504]\n",
      "loss: 12380.464844  [  6048/ 35504]\n",
      "loss: 13487.743164  [  6080/ 35504]\n",
      "loss: 15311.156250  [  6112/ 35504]\n",
      "loss: 13325.477539  [  6144/ 35504]\n",
      "loss: 13255.454102  [  6176/ 35504]\n",
      "loss: 12510.554688  [  6208/ 35504]\n",
      "loss: 13049.662109  [  6240/ 35504]\n",
      "loss: 11769.879883  [  6272/ 35504]\n",
      "loss: 13865.376953  [  6304/ 35504]\n",
      "loss: 13475.973633  [  6336/ 35504]\n",
      "loss: 14432.425781  [  6368/ 35504]\n",
      "loss: 13556.941406  [  6400/ 35504]\n",
      "loss: 12194.792969  [  6432/ 35504]\n",
      "loss: 11528.238281  [  6464/ 35504]\n",
      "loss: 13600.473633  [  6496/ 35504]\n",
      "loss: 11047.219727  [  6528/ 35504]\n",
      "loss: 10776.864258  [  6560/ 35504]\n",
      "loss: 17632.498047  [  6592/ 35504]\n",
      "loss: 11598.896484  [  6624/ 35504]\n",
      "loss: 11786.128906  [  6656/ 35504]\n",
      "loss: 11088.855469  [  6688/ 35504]\n",
      "loss: 14800.501953  [  6720/ 35504]\n",
      "loss: 10119.935547  [  6752/ 35504]\n",
      "loss: 13716.943359  [  6784/ 35504]\n",
      "loss: 12742.534180  [  6816/ 35504]\n",
      "loss: 14687.042969  [  6848/ 35504]\n",
      "loss: 10893.304688  [  6880/ 35504]\n",
      "loss: 11549.647461  [  6912/ 35504]\n",
      "loss: 12332.285156  [  6944/ 35504]\n",
      "loss: 14326.141602  [  6976/ 35504]\n",
      "loss: 12632.809570  [  7008/ 35504]\n",
      "loss: 9540.458008  [  7040/ 35504]\n",
      "loss: 12212.670898  [  7072/ 35504]\n",
      "loss: 12529.824219  [  7104/ 35504]\n",
      "loss: 9361.578125  [  7136/ 35504]\n",
      "loss: 12743.398438  [  7168/ 35504]\n",
      "loss: 13355.717773  [  7200/ 35504]\n",
      "loss: 13407.370117  [  7232/ 35504]\n",
      "loss: 14338.839844  [  7264/ 35504]\n",
      "loss: 14185.533203  [  7296/ 35504]\n",
      "loss: 16318.799805  [  7328/ 35504]\n",
      "loss: 9934.741211  [  7360/ 35504]\n",
      "loss: 12653.721680  [  7392/ 35504]\n",
      "loss: 12087.430664  [  7424/ 35504]\n",
      "loss: 10467.012695  [  7456/ 35504]\n",
      "loss: 12841.470703  [  7488/ 35504]\n",
      "loss: 13919.623047  [  7520/ 35504]\n",
      "loss: 10480.159180  [  7552/ 35504]\n",
      "loss: 9684.149414  [  7584/ 35504]\n",
      "loss: 10388.951172  [  7616/ 35504]\n",
      "loss: 10468.464844  [  7648/ 35504]\n",
      "loss: 12504.578125  [  7680/ 35504]\n",
      "loss: 10948.211914  [  7712/ 35504]\n",
      "loss: 12521.439453  [  7744/ 35504]\n",
      "loss: 10106.144531  [  7776/ 35504]\n",
      "loss: 8873.848633  [  7808/ 35504]\n",
      "loss: 11120.005859  [  7840/ 35504]\n",
      "loss: 11046.645508  [  7872/ 35504]\n",
      "loss: 13151.532227  [  7904/ 35504]\n",
      "loss: 11799.519531  [  7936/ 35504]\n",
      "loss: 12565.763672  [  7968/ 35504]\n",
      "loss: 12105.892578  [  8000/ 35504]\n",
      "loss: 10009.268555  [  8032/ 35504]\n",
      "loss: 10618.483398  [  8064/ 35504]\n",
      "loss: 9366.147461  [  8096/ 35504]\n",
      "loss: 8852.963867  [  8128/ 35504]\n",
      "loss: 12380.206055  [  8160/ 35504]\n",
      "loss: 9113.097656  [  8192/ 35504]\n",
      "loss: 13031.360352  [  8224/ 35504]\n",
      "loss: 10971.169922  [  8256/ 35504]\n",
      "loss: 11281.342773  [  8288/ 35504]\n",
      "loss: 12092.916016  [  8320/ 35504]\n",
      "loss: 11051.593750  [  8352/ 35504]\n",
      "loss: 10843.720703  [  8384/ 35504]\n",
      "loss: 12180.616211  [  8416/ 35504]\n",
      "loss: 12100.230469  [  8448/ 35504]\n",
      "loss: 11731.639648  [  8480/ 35504]\n",
      "loss: 10125.953125  [  8512/ 35504]\n",
      "loss: 8754.902344  [  8544/ 35504]\n",
      "loss: 12225.218750  [  8576/ 35504]\n",
      "loss: 10786.471680  [  8608/ 35504]\n",
      "loss: 8456.186523  [  8640/ 35504]\n",
      "loss: 8755.735352  [  8672/ 35504]\n",
      "loss: 7541.608887  [  8704/ 35504]\n",
      "loss: 9880.565430  [  8736/ 35504]\n",
      "loss: 11030.306641  [  8768/ 35504]\n",
      "loss: 11864.791992  [  8800/ 35504]\n",
      "loss: 10385.414062  [  8832/ 35504]\n",
      "loss: 11108.664062  [  8864/ 35504]\n",
      "loss: 8852.669922  [  8896/ 35504]\n",
      "loss: 12752.089844  [  8928/ 35504]\n",
      "loss: 6888.238770  [  8960/ 35504]\n",
      "loss: 8817.590820  [  8992/ 35504]\n",
      "loss: 8837.150391  [  9024/ 35504]\n",
      "loss: 8214.253906  [  9056/ 35504]\n",
      "loss: 11956.525391  [  9088/ 35504]\n",
      "loss: 11003.199219  [  9120/ 35504]\n",
      "loss: 8025.928711  [  9152/ 35504]\n",
      "loss: 7934.347656  [  9184/ 35504]\n",
      "loss: 9552.943359  [  9216/ 35504]\n",
      "loss: 11406.509766  [  9248/ 35504]\n",
      "loss: 11167.429688  [  9280/ 35504]\n",
      "loss: 9827.716797  [  9312/ 35504]\n",
      "loss: 10101.855469  [  9344/ 35504]\n",
      "loss: 8800.748047  [  9376/ 35504]\n",
      "loss: 9830.112305  [  9408/ 35504]\n",
      "loss: 10767.498047  [  9440/ 35504]\n",
      "loss: 10099.913086  [  9472/ 35504]\n",
      "loss: 10849.688477  [  9504/ 35504]\n",
      "loss: 8572.200195  [  9536/ 35504]\n",
      "loss: 7701.017090  [  9568/ 35504]\n",
      "loss: 9215.896484  [  9600/ 35504]\n",
      "loss: 11085.086914  [  9632/ 35504]\n",
      "loss: 8837.172852  [  9664/ 35504]\n",
      "loss: 8305.060547  [  9696/ 35504]\n",
      "loss: 8789.889648  [  9728/ 35504]\n",
      "loss: 10110.492188  [  9760/ 35504]\n",
      "loss: 9272.128906  [  9792/ 35504]\n",
      "loss: 10765.208984  [  9824/ 35504]\n",
      "loss: 9888.109375  [  9856/ 35504]\n",
      "loss: 11785.980469  [  9888/ 35504]\n",
      "loss: 10763.767578  [  9920/ 35504]\n",
      "loss: 10014.176758  [  9952/ 35504]\n",
      "loss: 8243.463867  [  9984/ 35504]\n",
      "loss: 10517.668945  [ 10016/ 35504]\n",
      "loss: 9850.910156  [ 10048/ 35504]\n",
      "loss: 9947.187500  [ 10080/ 35504]\n",
      "loss: 10217.085938  [ 10112/ 35504]\n",
      "loss: 7546.320801  [ 10144/ 35504]\n",
      "loss: 9327.245117  [ 10176/ 35504]\n",
      "loss: 8276.166016  [ 10208/ 35504]\n",
      "loss: 9093.433594  [ 10240/ 35504]\n",
      "loss: 9141.159180  [ 10272/ 35504]\n",
      "loss: 7012.564453  [ 10304/ 35504]\n",
      "loss: 10397.845703  [ 10336/ 35504]\n",
      "loss: 9850.474609  [ 10368/ 35504]\n",
      "loss: 9881.349609  [ 10400/ 35504]\n",
      "loss: 7671.035156  [ 10432/ 35504]\n",
      "loss: 11222.447266  [ 10464/ 35504]\n",
      "loss: 10369.882812  [ 10496/ 35504]\n",
      "loss: 9876.721680  [ 10528/ 35504]\n",
      "loss: 10461.442383  [ 10560/ 35504]\n",
      "loss: 7770.158691  [ 10592/ 35504]\n",
      "loss: 8307.855469  [ 10624/ 35504]\n",
      "loss: 8699.154297  [ 10656/ 35504]\n",
      "loss: 10418.490234  [ 10688/ 35504]\n",
      "loss: 8449.336914  [ 10720/ 35504]\n",
      "loss: 7906.245605  [ 10752/ 35504]\n",
      "loss: 10566.004883  [ 10784/ 35504]\n",
      "loss: 10355.290039  [ 10816/ 35504]\n",
      "loss: 7537.746094  [ 10848/ 35504]\n",
      "loss: 8525.401367  [ 10880/ 35504]\n",
      "loss: 8570.054688  [ 10912/ 35504]\n",
      "loss: 9869.623047  [ 10944/ 35504]\n",
      "loss: 9076.182617  [ 10976/ 35504]\n",
      "loss: 8373.511719  [ 11008/ 35504]\n",
      "loss: 9280.667969  [ 11040/ 35504]\n",
      "loss: 9474.968750  [ 11072/ 35504]\n",
      "loss: 10537.705078  [ 11104/ 35504]\n",
      "loss: 11443.817383  [ 11136/ 35504]\n",
      "loss: 9114.384766  [ 11168/ 35504]\n",
      "loss: 11132.484375  [ 11200/ 35504]\n",
      "loss: 7822.178223  [ 11232/ 35504]\n",
      "loss: 8169.086914  [ 11264/ 35504]\n",
      "loss: 6679.174316  [ 11296/ 35504]\n",
      "loss: 7702.407715  [ 11328/ 35504]\n",
      "loss: 8610.094727  [ 11360/ 35504]\n",
      "loss: 8124.462891  [ 11392/ 35504]\n",
      "loss: 7172.278809  [ 11424/ 35504]\n",
      "loss: 7140.713867  [ 11456/ 35504]\n",
      "loss: 7789.274414  [ 11488/ 35504]\n",
      "loss: 9006.312500  [ 11520/ 35504]\n",
      "loss: 6480.666992  [ 11552/ 35504]\n",
      "loss: 8861.904297  [ 11584/ 35504]\n",
      "loss: 6507.120117  [ 11616/ 35504]\n",
      "loss: 8815.598633  [ 11648/ 35504]\n",
      "loss: 9788.200195  [ 11680/ 35504]\n",
      "loss: 7123.593262  [ 11712/ 35504]\n",
      "loss: 4816.612305  [ 11744/ 35504]\n",
      "loss: 7712.733398  [ 11776/ 35504]\n",
      "loss: 9495.257812  [ 11808/ 35504]\n",
      "loss: 10150.768555  [ 11840/ 35504]\n",
      "loss: 8668.602539  [ 11872/ 35504]\n",
      "loss: 8609.575195  [ 11904/ 35504]\n",
      "loss: 8466.278320  [ 11936/ 35504]\n",
      "loss: 6475.310547  [ 11968/ 35504]\n",
      "loss: 7400.180176  [ 12000/ 35504]\n",
      "loss: 9461.207031  [ 12032/ 35504]\n",
      "loss: 8466.715820  [ 12064/ 35504]\n",
      "loss: 8105.636719  [ 12096/ 35504]\n",
      "loss: 7771.004883  [ 12128/ 35504]\n",
      "loss: 7399.732910  [ 12160/ 35504]\n",
      "loss: 8968.591797  [ 12192/ 35504]\n",
      "loss: 8316.588867  [ 12224/ 35504]\n",
      "loss: 8403.123047  [ 12256/ 35504]\n",
      "loss: 8793.498047  [ 12288/ 35504]\n",
      "loss: 5534.723633  [ 12320/ 35504]\n",
      "loss: 6149.653320  [ 12352/ 35504]\n",
      "loss: 7713.201172  [ 12384/ 35504]\n",
      "loss: 7091.424316  [ 12416/ 35504]\n",
      "loss: 6839.218262  [ 12448/ 35504]\n",
      "loss: 9064.591797  [ 12480/ 35504]\n",
      "loss: 6427.151855  [ 12512/ 35504]\n",
      "loss: 6722.813477  [ 12544/ 35504]\n",
      "loss: 7424.062012  [ 12576/ 35504]\n",
      "loss: 8386.741211  [ 12608/ 35504]\n",
      "loss: 6502.893555  [ 12640/ 35504]\n",
      "loss: 4221.325195  [ 12672/ 35504]\n",
      "loss: 5479.416016  [ 12704/ 35504]\n",
      "loss: 7397.241699  [ 12736/ 35504]\n",
      "loss: 8157.082520  [ 12768/ 35504]\n",
      "loss: 6133.900879  [ 12800/ 35504]\n",
      "loss: 5320.145020  [ 12832/ 35504]\n",
      "loss: 7492.072266  [ 12864/ 35504]\n",
      "loss: 7096.817383  [ 12896/ 35504]\n",
      "loss: 4291.630371  [ 12928/ 35504]\n",
      "loss: 7637.295898  [ 12960/ 35504]\n",
      "loss: 6390.793945  [ 12992/ 35504]\n",
      "loss: 6236.669922  [ 13024/ 35504]\n",
      "loss: 9155.324219  [ 13056/ 35504]\n",
      "loss: 4450.599121  [ 13088/ 35504]\n",
      "loss: 9049.943359  [ 13120/ 35504]\n",
      "loss: 7723.775879  [ 13152/ 35504]\n",
      "loss: 7381.069336  [ 13184/ 35504]\n",
      "loss: 5863.297852  [ 13216/ 35504]\n",
      "loss: 5627.466797  [ 13248/ 35504]\n",
      "loss: 7652.207031  [ 13280/ 35504]\n",
      "loss: 6600.296875  [ 13312/ 35504]\n",
      "loss: 7170.082031  [ 13344/ 35504]\n",
      "loss: 6293.802246  [ 13376/ 35504]\n",
      "loss: 6931.709473  [ 13408/ 35504]\n",
      "loss: 6172.631348  [ 13440/ 35504]\n",
      "loss: 6399.294434  [ 13472/ 35504]\n",
      "loss: 5967.106934  [ 13504/ 35504]\n",
      "loss: 8306.685547  [ 13536/ 35504]\n",
      "loss: 7172.607910  [ 13568/ 35504]\n",
      "loss: 5340.752930  [ 13600/ 35504]\n",
      "loss: 7466.392578  [ 13632/ 35504]\n",
      "loss: 7165.756836  [ 13664/ 35504]\n",
      "loss: 7364.696777  [ 13696/ 35504]\n",
      "loss: 4595.666016  [ 13728/ 35504]\n",
      "loss: 6197.186523  [ 13760/ 35504]\n",
      "loss: 5437.561523  [ 13792/ 35504]\n",
      "loss: 6780.758301  [ 13824/ 35504]\n",
      "loss: 5940.144531  [ 13856/ 35504]\n",
      "loss: 6178.020020  [ 13888/ 35504]\n",
      "loss: 5332.109375  [ 13920/ 35504]\n",
      "loss: 9109.521484  [ 13952/ 35504]\n",
      "loss: 4912.941406  [ 13984/ 35504]\n",
      "loss: 8395.250977  [ 14016/ 35504]\n",
      "loss: 6144.683594  [ 14048/ 35504]\n",
      "loss: 6115.828125  [ 14080/ 35504]\n",
      "loss: 6628.271973  [ 14112/ 35504]\n",
      "loss: 5122.794922  [ 14144/ 35504]\n",
      "loss: 9684.914062  [ 14176/ 35504]\n",
      "loss: 6348.062500  [ 14208/ 35504]\n",
      "loss: 5848.645508  [ 14240/ 35504]\n",
      "loss: 6475.939453  [ 14272/ 35504]\n",
      "loss: 6026.027832  [ 14304/ 35504]\n",
      "loss: 5605.109375  [ 14336/ 35504]\n",
      "loss: 6263.399414  [ 14368/ 35504]\n",
      "loss: 7304.068848  [ 14400/ 35504]\n",
      "loss: 8988.488281  [ 14432/ 35504]\n",
      "loss: 5304.901367  [ 14464/ 35504]\n",
      "loss: 6874.529297  [ 14496/ 35504]\n",
      "loss: 7517.805664  [ 14528/ 35504]\n",
      "loss: 7125.624023  [ 14560/ 35504]\n",
      "loss: 6486.439453  [ 14592/ 35504]\n",
      "loss: 6328.998535  [ 14624/ 35504]\n",
      "loss: 6606.000488  [ 14656/ 35504]\n",
      "loss: 5344.351074  [ 14688/ 35504]\n",
      "loss: 6454.208984  [ 14720/ 35504]\n",
      "loss: 6756.024414  [ 14752/ 35504]\n",
      "loss: 7460.810547  [ 14784/ 35504]\n",
      "loss: 5832.536133  [ 14816/ 35504]\n",
      "loss: 6478.787109  [ 14848/ 35504]\n",
      "loss: 4590.666992  [ 14880/ 35504]\n",
      "loss: 5337.170898  [ 14912/ 35504]\n",
      "loss: 5250.755859  [ 14944/ 35504]\n",
      "loss: 6440.304199  [ 14976/ 35504]\n",
      "loss: 5556.542480  [ 15008/ 35504]\n",
      "loss: 7428.583496  [ 15040/ 35504]\n",
      "loss: 4813.510742  [ 15072/ 35504]\n",
      "loss: 8452.235352  [ 15104/ 35504]\n",
      "loss: 5203.665039  [ 15136/ 35504]\n",
      "loss: 6718.413086  [ 15168/ 35504]\n",
      "loss: 5582.268555  [ 15200/ 35504]\n",
      "loss: 7366.823730  [ 15232/ 35504]\n",
      "loss: 6735.226074  [ 15264/ 35504]\n",
      "loss: 5665.834473  [ 15296/ 35504]\n",
      "loss: 7528.204590  [ 15328/ 35504]\n",
      "loss: 4824.015625  [ 15360/ 35504]\n",
      "loss: 5928.688477  [ 15392/ 35504]\n",
      "loss: 6327.394043  [ 15424/ 35504]\n",
      "loss: 4018.563721  [ 15456/ 35504]\n",
      "loss: 5986.638672  [ 15488/ 35504]\n",
      "loss: 5869.222656  [ 15520/ 35504]\n",
      "loss: 4766.798340  [ 15552/ 35504]\n",
      "loss: 5292.849121  [ 15584/ 35504]\n",
      "loss: 6405.677246  [ 15616/ 35504]\n",
      "loss: 4343.505371  [ 15648/ 35504]\n",
      "loss: 4962.500488  [ 15680/ 35504]\n",
      "loss: 7594.950684  [ 15712/ 35504]\n",
      "loss: 4813.688477  [ 15744/ 35504]\n",
      "loss: 5042.211914  [ 15776/ 35504]\n",
      "loss: 5819.536621  [ 15808/ 35504]\n",
      "loss: 5635.207031  [ 15840/ 35504]\n",
      "loss: 7320.443848  [ 15872/ 35504]\n",
      "loss: 5736.133789  [ 15904/ 35504]\n",
      "loss: 5580.150391  [ 15936/ 35504]\n",
      "loss: 6071.505371  [ 15968/ 35504]\n",
      "loss: 7271.557617  [ 16000/ 35504]\n",
      "loss: 6368.921875  [ 16032/ 35504]\n",
      "loss: 5376.633789  [ 16064/ 35504]\n",
      "loss: 5279.267090  [ 16096/ 35504]\n",
      "loss: 5467.748047  [ 16128/ 35504]\n",
      "loss: 6097.926758  [ 16160/ 35504]\n",
      "loss: 4662.488281  [ 16192/ 35504]\n",
      "loss: 5280.374023  [ 16224/ 35504]\n",
      "loss: 4643.105469  [ 16256/ 35504]\n",
      "loss: 3274.478760  [ 16288/ 35504]\n",
      "loss: 4952.707031  [ 16320/ 35504]\n",
      "loss: 5928.801270  [ 16352/ 35504]\n",
      "loss: 6037.844727  [ 16384/ 35504]\n",
      "loss: 6759.134766  [ 16416/ 35504]\n",
      "loss: 5798.272461  [ 16448/ 35504]\n",
      "loss: 5954.950684  [ 16480/ 35504]\n",
      "loss: 5562.253418  [ 16512/ 35504]\n",
      "loss: 3730.007812  [ 16544/ 35504]\n",
      "loss: 4677.490723  [ 16576/ 35504]\n",
      "loss: 3931.311768  [ 16608/ 35504]\n",
      "loss: 5890.929688  [ 16640/ 35504]\n",
      "loss: 3827.703125  [ 16672/ 35504]\n",
      "loss: 4029.429688  [ 16704/ 35504]\n",
      "loss: 5238.432617  [ 16736/ 35504]\n",
      "loss: 6529.143555  [ 16768/ 35504]\n",
      "loss: 6169.046875  [ 16800/ 35504]\n",
      "loss: 6289.552246  [ 16832/ 35504]\n",
      "loss: 6230.518066  [ 16864/ 35504]\n",
      "loss: 5244.355469  [ 16896/ 35504]\n",
      "loss: 3573.677002  [ 16928/ 35504]\n",
      "loss: 5135.770996  [ 16960/ 35504]\n",
      "loss: 3037.276367  [ 16992/ 35504]\n",
      "loss: 5344.790039  [ 17024/ 35504]\n",
      "loss: 5247.498535  [ 17056/ 35504]\n",
      "loss: 5406.161621  [ 17088/ 35504]\n",
      "loss: 4436.996582  [ 17120/ 35504]\n",
      "loss: 5430.805176  [ 17152/ 35504]\n",
      "loss: 6858.857422  [ 17184/ 35504]\n",
      "loss: 4526.680664  [ 17216/ 35504]\n",
      "loss: 4471.200195  [ 17248/ 35504]\n",
      "loss: 4296.101074  [ 17280/ 35504]\n",
      "loss: 3910.066650  [ 17312/ 35504]\n",
      "loss: 3973.060547  [ 17344/ 35504]\n",
      "loss: 4882.918457  [ 17376/ 35504]\n",
      "loss: 5579.397949  [ 17408/ 35504]\n",
      "loss: 6356.237305  [ 17440/ 35504]\n",
      "loss: 4704.553223  [ 17472/ 35504]\n",
      "loss: 4301.699707  [ 17504/ 35504]\n",
      "loss: 5718.333984  [ 17536/ 35504]\n",
      "loss: 5497.618164  [ 17568/ 35504]\n",
      "loss: 6511.493164  [ 17600/ 35504]\n",
      "loss: 5223.498047  [ 17632/ 35504]\n",
      "loss: 4165.459961  [ 17664/ 35504]\n",
      "loss: 4367.291504  [ 17696/ 35504]\n",
      "loss: 5552.494629  [ 17728/ 35504]\n",
      "loss: 4140.158691  [ 17760/ 35504]\n",
      "loss: 4768.371094  [ 17792/ 35504]\n",
      "loss: 5100.627441  [ 17824/ 35504]\n",
      "loss: 4787.258301  [ 17856/ 35504]\n",
      "loss: 3664.790527  [ 17888/ 35504]\n",
      "loss: 5450.323730  [ 17920/ 35504]\n",
      "loss: 4361.583984  [ 17952/ 35504]\n",
      "loss: 6846.923340  [ 17984/ 35504]\n",
      "loss: 5255.121582  [ 18016/ 35504]\n",
      "loss: 4496.627441  [ 18048/ 35504]\n",
      "loss: 7452.760254  [ 18080/ 35504]\n",
      "loss: 3771.819092  [ 18112/ 35504]\n",
      "loss: 5421.202637  [ 18144/ 35504]\n",
      "loss: 4935.857422  [ 18176/ 35504]\n",
      "loss: 4248.501465  [ 18208/ 35504]\n",
      "loss: 2789.636719  [ 18240/ 35504]\n",
      "loss: 4375.531250  [ 18272/ 35504]\n",
      "loss: 5185.639648  [ 18304/ 35504]\n",
      "loss: 5745.756348  [ 18336/ 35504]\n",
      "loss: 4768.863281  [ 18368/ 35504]\n",
      "loss: 4111.708496  [ 18400/ 35504]\n",
      "loss: 4287.149414  [ 18432/ 35504]\n",
      "loss: 4393.094238  [ 18464/ 35504]\n",
      "loss: 4491.849609  [ 18496/ 35504]\n",
      "loss: 3948.304688  [ 18528/ 35504]\n",
      "loss: 3271.051514  [ 18560/ 35504]\n",
      "loss: 4299.672852  [ 18592/ 35504]\n",
      "loss: 5796.720215  [ 18624/ 35504]\n",
      "loss: 6108.604004  [ 18656/ 35504]\n",
      "loss: 5143.851074  [ 18688/ 35504]\n",
      "loss: 6009.193359  [ 18720/ 35504]\n",
      "loss: 5116.493164  [ 18752/ 35504]\n",
      "loss: 4244.547363  [ 18784/ 35504]\n",
      "loss: 3458.659668  [ 18816/ 35504]\n",
      "loss: 3743.070801  [ 18848/ 35504]\n",
      "loss: 5575.434570  [ 18880/ 35504]\n",
      "loss: 4410.632812  [ 18912/ 35504]\n",
      "loss: 5103.059082  [ 18944/ 35504]\n",
      "loss: 3670.755371  [ 18976/ 35504]\n",
      "loss: 4561.532227  [ 19008/ 35504]\n",
      "loss: 4746.376465  [ 19040/ 35504]\n",
      "loss: 4905.745117  [ 19072/ 35504]\n",
      "loss: 4422.672363  [ 19104/ 35504]\n",
      "loss: 6112.750488  [ 19136/ 35504]\n",
      "loss: 3812.313232  [ 19168/ 35504]\n",
      "loss: 3359.590332  [ 19200/ 35504]\n",
      "loss: 3940.196533  [ 19232/ 35504]\n",
      "loss: 4760.304199  [ 19264/ 35504]\n",
      "loss: 4556.471680  [ 19296/ 35504]\n",
      "loss: 2812.850098  [ 19328/ 35504]\n",
      "loss: 3964.851318  [ 19360/ 35504]\n",
      "loss: 3126.333008  [ 19392/ 35504]\n",
      "loss: 2869.595215  [ 19424/ 35504]\n",
      "loss: 6274.587891  [ 19456/ 35504]\n",
      "loss: 2408.105957  [ 19488/ 35504]\n",
      "loss: 2822.267578  [ 19520/ 35504]\n",
      "loss: 3973.826660  [ 19552/ 35504]\n",
      "loss: 3419.075684  [ 19584/ 35504]\n",
      "loss: 3605.979492  [ 19616/ 35504]\n",
      "loss: 5343.595703  [ 19648/ 35504]\n",
      "loss: 4129.644531  [ 19680/ 35504]\n",
      "loss: 4419.660645  [ 19712/ 35504]\n",
      "loss: 3632.463623  [ 19744/ 35504]\n",
      "loss: 3124.903320  [ 19776/ 35504]\n",
      "loss: 4019.386475  [ 19808/ 35504]\n",
      "loss: 2619.308838  [ 19840/ 35504]\n",
      "loss: 3228.566895  [ 19872/ 35504]\n",
      "loss: 4181.574707  [ 19904/ 35504]\n",
      "loss: 5302.818359  [ 19936/ 35504]\n",
      "loss: 4100.813477  [ 19968/ 35504]\n",
      "loss: 3453.475098  [ 20000/ 35504]\n",
      "loss: 4369.466309  [ 20032/ 35504]\n",
      "loss: 4855.274414  [ 20064/ 35504]\n",
      "loss: 4380.996582  [ 20096/ 35504]\n",
      "loss: 2893.632812  [ 20128/ 35504]\n",
      "loss: 2532.025879  [ 20160/ 35504]\n",
      "loss: 4975.166992  [ 20192/ 35504]\n",
      "loss: 4478.546875  [ 20224/ 35504]\n",
      "loss: 3858.020508  [ 20256/ 35504]\n",
      "loss: 3712.856201  [ 20288/ 35504]\n",
      "loss: 4513.402832  [ 20320/ 35504]\n",
      "loss: 4061.696533  [ 20352/ 35504]\n",
      "loss: 3478.411621  [ 20384/ 35504]\n",
      "loss: 3792.793457  [ 20416/ 35504]\n",
      "loss: 3974.789795  [ 20448/ 35504]\n",
      "loss: 4066.917969  [ 20480/ 35504]\n",
      "loss: 2224.147949  [ 20512/ 35504]\n",
      "loss: 5527.768066  [ 20544/ 35504]\n",
      "loss: 3576.850098  [ 20576/ 35504]\n",
      "loss: 4442.390137  [ 20608/ 35504]\n",
      "loss: 3204.026123  [ 20640/ 35504]\n",
      "loss: 5216.392090  [ 20672/ 35504]\n",
      "loss: 5272.297363  [ 20704/ 35504]\n",
      "loss: 3817.652344  [ 20736/ 35504]\n",
      "loss: 1943.230347  [ 20768/ 35504]\n",
      "loss: 3664.061768  [ 20800/ 35504]\n",
      "loss: 4528.988770  [ 20832/ 35504]\n",
      "loss: 3802.773438  [ 20864/ 35504]\n",
      "loss: 3513.218750  [ 20896/ 35504]\n",
      "loss: 2564.715332  [ 20928/ 35504]\n",
      "loss: 3987.353516  [ 20960/ 35504]\n",
      "loss: 4473.977051  [ 20992/ 35504]\n",
      "loss: 4175.754883  [ 21024/ 35504]\n",
      "loss: 4304.016602  [ 21056/ 35504]\n",
      "loss: 3568.270508  [ 21088/ 35504]\n",
      "loss: 3223.521729  [ 21120/ 35504]\n",
      "loss: 3895.573486  [ 21152/ 35504]\n",
      "loss: 2788.650879  [ 21184/ 35504]\n",
      "loss: 4125.421387  [ 21216/ 35504]\n",
      "loss: 3561.963623  [ 21248/ 35504]\n",
      "loss: 4766.770020  [ 21280/ 35504]\n",
      "loss: 2424.693604  [ 21312/ 35504]\n",
      "loss: 3775.527344  [ 21344/ 35504]\n",
      "loss: 4135.887207  [ 21376/ 35504]\n",
      "loss: 3507.258545  [ 21408/ 35504]\n",
      "loss: 3798.597168  [ 21440/ 35504]\n",
      "loss: 5619.291992  [ 21472/ 35504]\n",
      "loss: 5247.642578  [ 21504/ 35504]\n",
      "loss: 2929.131104  [ 21536/ 35504]\n",
      "loss: 3392.799561  [ 21568/ 35504]\n",
      "loss: 3862.151367  [ 21600/ 35504]\n",
      "loss: 3683.232422  [ 21632/ 35504]\n",
      "loss: 3286.084229  [ 21664/ 35504]\n",
      "loss: 4266.780273  [ 21696/ 35504]\n",
      "loss: 3649.197754  [ 21728/ 35504]\n",
      "loss: 2801.421387  [ 21760/ 35504]\n",
      "loss: 4003.622803  [ 21792/ 35504]\n",
      "loss: 5174.679688  [ 21824/ 35504]\n",
      "loss: 3146.400146  [ 21856/ 35504]\n",
      "loss: 2584.672119  [ 21888/ 35504]\n",
      "loss: 4080.380859  [ 21920/ 35504]\n",
      "loss: 2782.591553  [ 21952/ 35504]\n",
      "loss: 4365.446289  [ 21984/ 35504]\n",
      "loss: 2671.733643  [ 22016/ 35504]\n",
      "loss: 3592.858398  [ 22048/ 35504]\n",
      "loss: 2955.560547  [ 22080/ 35504]\n",
      "loss: 2413.288086  [ 22112/ 35504]\n",
      "loss: 2910.663818  [ 22144/ 35504]\n",
      "loss: 4289.545410  [ 22176/ 35504]\n",
      "loss: 3562.503662  [ 22208/ 35504]\n",
      "loss: 3126.538330  [ 22240/ 35504]\n",
      "loss: 3794.677734  [ 22272/ 35504]\n",
      "loss: 3493.655273  [ 22304/ 35504]\n",
      "loss: 3389.145996  [ 22336/ 35504]\n",
      "loss: 3432.462646  [ 22368/ 35504]\n",
      "loss: 3454.189453  [ 22400/ 35504]\n",
      "loss: 3013.343262  [ 22432/ 35504]\n",
      "loss: 2649.050537  [ 22464/ 35504]\n",
      "loss: 1694.212402  [ 22496/ 35504]\n",
      "loss: 3598.819580  [ 22528/ 35504]\n",
      "loss: 3425.412109  [ 22560/ 35504]\n",
      "loss: 5012.607422  [ 22592/ 35504]\n",
      "loss: 3578.535156  [ 22624/ 35504]\n",
      "loss: 3726.186768  [ 22656/ 35504]\n",
      "loss: 3459.887207  [ 22688/ 35504]\n",
      "loss: 4102.596191  [ 22720/ 35504]\n",
      "loss: 4768.406738  [ 22752/ 35504]\n",
      "loss: 3036.050537  [ 22784/ 35504]\n",
      "loss: 2696.021973  [ 22816/ 35504]\n",
      "loss: 3557.112061  [ 22848/ 35504]\n",
      "loss: 4512.087891  [ 22880/ 35504]\n",
      "loss: 3302.408447  [ 22912/ 35504]\n",
      "loss: 2866.542725  [ 22944/ 35504]\n",
      "loss: 3184.207520  [ 22976/ 35504]\n",
      "loss: 2838.435791  [ 23008/ 35504]\n",
      "loss: 3595.312500  [ 23040/ 35504]\n",
      "loss: 2266.301758  [ 23072/ 35504]\n",
      "loss: 2814.784912  [ 23104/ 35504]\n",
      "loss: 2147.579834  [ 23136/ 35504]\n",
      "loss: 3367.762695  [ 23168/ 35504]\n",
      "loss: 3067.302490  [ 23200/ 35504]\n",
      "loss: 2634.910156  [ 23232/ 35504]\n",
      "loss: 3233.934814  [ 23264/ 35504]\n",
      "loss: 2392.562500  [ 23296/ 35504]\n",
      "loss: 2772.048584  [ 23328/ 35504]\n",
      "loss: 3241.726562  [ 23360/ 35504]\n",
      "loss: 3325.739014  [ 23392/ 35504]\n",
      "loss: 3494.438232  [ 23424/ 35504]\n",
      "loss: 2368.428711  [ 23456/ 35504]\n",
      "loss: 3480.792480  [ 23488/ 35504]\n",
      "loss: 3931.653076  [ 23520/ 35504]\n",
      "loss: 2916.231934  [ 23552/ 35504]\n",
      "loss: 4708.306641  [ 23584/ 35504]\n",
      "loss: 3064.037109  [ 23616/ 35504]\n",
      "loss: 2427.212158  [ 23648/ 35504]\n",
      "loss: 1836.305298  [ 23680/ 35504]\n",
      "loss: 3154.548828  [ 23712/ 35504]\n",
      "loss: 2503.621338  [ 23744/ 35504]\n",
      "loss: 2324.505127  [ 23776/ 35504]\n",
      "loss: 3035.374512  [ 23808/ 35504]\n",
      "loss: 2947.804688  [ 23840/ 35504]\n",
      "loss: 3304.697021  [ 23872/ 35504]\n",
      "loss: 3501.586914  [ 23904/ 35504]\n",
      "loss: 2095.874268  [ 23936/ 35504]\n",
      "loss: 4677.119629  [ 23968/ 35504]\n",
      "loss: 2785.530273  [ 24000/ 35504]\n",
      "loss: 3313.163086  [ 24032/ 35504]\n",
      "loss: 2681.273682  [ 24064/ 35504]\n",
      "loss: 2072.279053  [ 24096/ 35504]\n",
      "loss: 2205.108643  [ 24128/ 35504]\n",
      "loss: 2258.486084  [ 24160/ 35504]\n",
      "loss: 2693.166504  [ 24192/ 35504]\n",
      "loss: 2241.176025  [ 24224/ 35504]\n",
      "loss: 2278.661865  [ 24256/ 35504]\n",
      "loss: 1979.455566  [ 24288/ 35504]\n",
      "loss: 3167.857422  [ 24320/ 35504]\n",
      "loss: 1857.979736  [ 24352/ 35504]\n",
      "loss: 2183.671143  [ 24384/ 35504]\n",
      "loss: 2349.033203  [ 24416/ 35504]\n",
      "loss: 2631.457275  [ 24448/ 35504]\n",
      "loss: 2332.381104  [ 24480/ 35504]\n",
      "loss: 2864.572998  [ 24512/ 35504]\n",
      "loss: 2306.882568  [ 24544/ 35504]\n",
      "loss: 3361.072266  [ 24576/ 35504]\n",
      "loss: 1769.929565  [ 24608/ 35504]\n",
      "loss: 3048.472656  [ 24640/ 35504]\n",
      "loss: 2815.619385  [ 24672/ 35504]\n",
      "loss: 2651.837158  [ 24704/ 35504]\n",
      "loss: 2354.564941  [ 24736/ 35504]\n",
      "loss: 3370.260254  [ 24768/ 35504]\n",
      "loss: 2479.750000  [ 24800/ 35504]\n",
      "loss: 2065.824219  [ 24832/ 35504]\n",
      "loss: 2417.354248  [ 24864/ 35504]\n",
      "loss: 3978.899414  [ 24896/ 35504]\n",
      "loss: 2267.064697  [ 24928/ 35504]\n",
      "loss: 2528.500488  [ 24960/ 35504]\n",
      "loss: 3184.967529  [ 24992/ 35504]\n",
      "loss: 2713.039795  [ 25024/ 35504]\n",
      "loss: 1331.292725  [ 25056/ 35504]\n",
      "loss: 2452.955566  [ 25088/ 35504]\n",
      "loss: 2199.987305  [ 25120/ 35504]\n",
      "loss: 2852.729004  [ 25152/ 35504]\n",
      "loss: 3572.006104  [ 25184/ 35504]\n",
      "loss: 1755.144043  [ 25216/ 35504]\n",
      "loss: 3503.689941  [ 25248/ 35504]\n",
      "loss: 2336.972900  [ 25280/ 35504]\n",
      "loss: 2536.470215  [ 25312/ 35504]\n",
      "loss: 2293.060059  [ 25344/ 35504]\n",
      "loss: 1225.841675  [ 25376/ 35504]\n",
      "loss: 2561.470947  [ 25408/ 35504]\n",
      "loss: 2570.500977  [ 25440/ 35504]\n",
      "loss: 2321.906006  [ 25472/ 35504]\n",
      "loss: 2039.484253  [ 25504/ 35504]\n",
      "loss: 2242.310303  [ 25536/ 35504]\n",
      "loss: 2010.708984  [ 25568/ 35504]\n",
      "loss: 2943.865967  [ 25600/ 35504]\n",
      "loss: 1789.334106  [ 25632/ 35504]\n",
      "loss: 3001.967529  [ 25664/ 35504]\n",
      "loss: 2428.218018  [ 25696/ 35504]\n",
      "loss: 2042.223633  [ 25728/ 35504]\n",
      "loss: 3349.170898  [ 25760/ 35504]\n",
      "loss: 2135.646240  [ 25792/ 35504]\n",
      "loss: 2875.830566  [ 25824/ 35504]\n",
      "loss: 2141.561768  [ 25856/ 35504]\n",
      "loss: 2510.221436  [ 25888/ 35504]\n",
      "loss: 1697.608398  [ 25920/ 35504]\n",
      "loss: 2371.814941  [ 25952/ 35504]\n",
      "loss: 2207.688477  [ 25984/ 35504]\n",
      "loss: 3379.295166  [ 26016/ 35504]\n",
      "loss: 2811.237549  [ 26048/ 35504]\n",
      "loss: 2779.622559  [ 26080/ 35504]\n",
      "loss: 3501.223389  [ 26112/ 35504]\n",
      "loss: 2757.679199  [ 26144/ 35504]\n",
      "loss: 2102.471680  [ 26176/ 35504]\n",
      "loss: 3199.588623  [ 26208/ 35504]\n",
      "loss: 2567.177734  [ 26240/ 35504]\n",
      "loss: 2542.588867  [ 26272/ 35504]\n",
      "loss: 2282.666504  [ 26304/ 35504]\n",
      "loss: 4130.008301  [ 26336/ 35504]\n",
      "loss: 3100.677979  [ 26368/ 35504]\n",
      "loss: 1754.060913  [ 26400/ 35504]\n",
      "loss: 1806.510620  [ 26432/ 35504]\n",
      "loss: 2917.606689  [ 26464/ 35504]\n",
      "loss: 2042.584106  [ 26496/ 35504]\n",
      "loss: 3075.289307  [ 26528/ 35504]\n",
      "loss: 3441.666748  [ 26560/ 35504]\n",
      "loss: 1647.946899  [ 26592/ 35504]\n",
      "loss: 1279.113403  [ 26624/ 35504]\n",
      "loss: 1356.040771  [ 26656/ 35504]\n",
      "loss: 2454.400635  [ 26688/ 35504]\n",
      "loss: 2412.216553  [ 26720/ 35504]\n",
      "loss: 2318.395508  [ 26752/ 35504]\n",
      "loss: 2856.617676  [ 26784/ 35504]\n",
      "loss: 2144.127197  [ 26816/ 35504]\n",
      "loss: 1943.470337  [ 26848/ 35504]\n",
      "loss: 3176.071045  [ 26880/ 35504]\n",
      "loss: 2554.602295  [ 26912/ 35504]\n",
      "loss: 2792.644775  [ 26944/ 35504]\n",
      "loss: 1655.899658  [ 26976/ 35504]\n",
      "loss: 2132.908936  [ 27008/ 35504]\n",
      "loss: 2523.148926  [ 27040/ 35504]\n",
      "loss: 2254.618896  [ 27072/ 35504]\n",
      "loss: 1916.055542  [ 27104/ 35504]\n",
      "loss: 2581.478760  [ 27136/ 35504]\n",
      "loss: 1245.404785  [ 27168/ 35504]\n",
      "loss: 1522.826904  [ 27200/ 35504]\n",
      "loss: 2661.549072  [ 27232/ 35504]\n",
      "loss: 2847.366699  [ 27264/ 35504]\n",
      "loss: 2250.452393  [ 27296/ 35504]\n",
      "loss: 3467.456543  [ 27328/ 35504]\n",
      "loss: 2124.884033  [ 27360/ 35504]\n",
      "loss: 2124.528320  [ 27392/ 35504]\n",
      "loss: 2855.062744  [ 27424/ 35504]\n",
      "loss: 2031.027954  [ 27456/ 35504]\n",
      "loss: 2134.580811  [ 27488/ 35504]\n",
      "loss: 2457.491943  [ 27520/ 35504]\n",
      "loss: 2300.327148  [ 27552/ 35504]\n",
      "loss: 1321.702148  [ 27584/ 35504]\n",
      "loss: 1786.178223  [ 27616/ 35504]\n",
      "loss: 2540.638916  [ 27648/ 35504]\n",
      "loss: 2074.407227  [ 27680/ 35504]\n",
      "loss: 3805.865723  [ 27712/ 35504]\n",
      "loss: 2475.375732  [ 27744/ 35504]\n",
      "loss: 2612.456543  [ 27776/ 35504]\n",
      "loss: 2340.279053  [ 27808/ 35504]\n",
      "loss: 1879.729736  [ 27840/ 35504]\n",
      "loss: 1972.900269  [ 27872/ 35504]\n",
      "loss: 2310.950195  [ 27904/ 35504]\n",
      "loss: 2222.571289  [ 27936/ 35504]\n",
      "loss: 2128.662354  [ 27968/ 35504]\n",
      "loss: 1556.093140  [ 28000/ 35504]\n",
      "loss: 1793.775635  [ 28032/ 35504]\n",
      "loss: 1804.619629  [ 28064/ 35504]\n",
      "loss: 2663.926270  [ 28096/ 35504]\n",
      "loss: 1702.095093  [ 28128/ 35504]\n",
      "loss: 2082.238770  [ 28160/ 35504]\n",
      "loss: 2274.776367  [ 28192/ 35504]\n",
      "loss: 2175.868164  [ 28224/ 35504]\n",
      "loss: 2142.726074  [ 28256/ 35504]\n",
      "loss: 1351.453979  [ 28288/ 35504]\n",
      "loss: 2267.946533  [ 28320/ 35504]\n",
      "loss: 2439.650146  [ 28352/ 35504]\n",
      "loss: 2503.544189  [ 28384/ 35504]\n",
      "loss: 2926.597900  [ 28416/ 35504]\n",
      "loss: 2646.016602  [ 28448/ 35504]\n",
      "loss: 2629.808105  [ 28480/ 35504]\n",
      "loss: 2248.371582  [ 28512/ 35504]\n",
      "loss: 1731.586060  [ 28544/ 35504]\n",
      "loss: 1972.925415  [ 28576/ 35504]\n",
      "loss: 1232.587036  [ 28608/ 35504]\n",
      "loss: 2855.910156  [ 28640/ 35504]\n",
      "loss: 1320.580566  [ 28672/ 35504]\n",
      "loss: 2342.575439  [ 28704/ 35504]\n",
      "loss: 1607.963867  [ 28736/ 35504]\n",
      "loss: 2003.750488  [ 28768/ 35504]\n",
      "loss: 1702.373291  [ 28800/ 35504]\n",
      "loss: 1281.217285  [ 28832/ 35504]\n",
      "loss: 1676.849976  [ 28864/ 35504]\n",
      "loss: 1698.093384  [ 28896/ 35504]\n",
      "loss: 2017.835327  [ 28928/ 35504]\n",
      "loss: 2429.563477  [ 28960/ 35504]\n",
      "loss: 2696.243164  [ 28992/ 35504]\n",
      "loss: 1561.379639  [ 29024/ 35504]\n",
      "loss: 2982.112305  [ 29056/ 35504]\n",
      "loss: 1258.108765  [ 29088/ 35504]\n",
      "loss: 1838.683594  [ 29120/ 35504]\n",
      "loss: 2318.309570  [ 29152/ 35504]\n",
      "loss: 1891.979492  [ 29184/ 35504]\n",
      "loss: 1851.229858  [ 29216/ 35504]\n",
      "loss: 1569.225464  [ 29248/ 35504]\n",
      "loss: 1616.453491  [ 29280/ 35504]\n",
      "loss: 2790.499756  [ 29312/ 35504]\n",
      "loss: 2259.403809  [ 29344/ 35504]\n",
      "loss: 1156.556030  [ 29376/ 35504]\n",
      "loss: 1368.493652  [ 29408/ 35504]\n",
      "loss: 1735.440918  [ 29440/ 35504]\n",
      "loss: 1497.655273  [ 29472/ 35504]\n",
      "loss: 1419.937378  [ 29504/ 35504]\n",
      "loss: 1217.571777  [ 29536/ 35504]\n",
      "loss: 1998.161255  [ 29568/ 35504]\n",
      "loss: 2374.345459  [ 29600/ 35504]\n",
      "loss: 1600.725342  [ 29632/ 35504]\n",
      "loss: 1438.183105  [ 29664/ 35504]\n",
      "loss: 1734.903076  [ 29696/ 35504]\n",
      "loss: 1357.307495  [ 29728/ 35504]\n",
      "loss: 2407.277100  [ 29760/ 35504]\n",
      "loss: 1406.364136  [ 29792/ 35504]\n",
      "loss: 1378.885010  [ 29824/ 35504]\n",
      "loss: 1027.738281  [ 29856/ 35504]\n",
      "loss: 1863.470337  [ 29888/ 35504]\n",
      "loss: 1960.647095  [ 29920/ 35504]\n",
      "loss: 1762.193237  [ 29952/ 35504]\n",
      "loss: 1610.299927  [ 29984/ 35504]\n",
      "loss: 1460.239014  [ 30016/ 35504]\n",
      "loss: 2217.040039  [ 30048/ 35504]\n",
      "loss: 3102.000000  [ 30080/ 35504]\n",
      "loss: 1684.069580  [ 30112/ 35504]\n",
      "loss: 1960.938965  [ 30144/ 35504]\n",
      "loss: 1971.429932  [ 30176/ 35504]\n",
      "loss: 1718.979736  [ 30208/ 35504]\n",
      "loss: 2416.334473  [ 30240/ 35504]\n",
      "loss: 1370.429565  [ 30272/ 35504]\n",
      "loss: 1771.137329  [ 30304/ 35504]\n",
      "loss: 1653.193970  [ 30336/ 35504]\n",
      "loss: 1300.255981  [ 30368/ 35504]\n",
      "loss: 1246.563232  [ 30400/ 35504]\n",
      "loss: 1979.559204  [ 30432/ 35504]\n",
      "loss: 2162.762207  [ 30464/ 35504]\n",
      "loss: 1985.937012  [ 30496/ 35504]\n",
      "loss: 1185.848755  [ 30528/ 35504]\n",
      "loss: 1620.103516  [ 30560/ 35504]\n",
      "loss: 1473.052002  [ 30592/ 35504]\n",
      "loss: 2050.659668  [ 30624/ 35504]\n",
      "loss: 2134.800049  [ 30656/ 35504]\n",
      "loss: 1769.206543  [ 30688/ 35504]\n",
      "loss: 1371.875977  [ 30720/ 35504]\n",
      "loss: 2019.797852  [ 30752/ 35504]\n",
      "loss: 1548.694458  [ 30784/ 35504]\n",
      "loss: 2315.470459  [ 30816/ 35504]\n",
      "loss: 1699.446899  [ 30848/ 35504]\n",
      "loss: 1109.544067  [ 30880/ 35504]\n",
      "loss: 1658.813721  [ 30912/ 35504]\n",
      "loss: 1599.880493  [ 30944/ 35504]\n",
      "loss: 1632.616089  [ 30976/ 35504]\n",
      "loss: 1840.508057  [ 31008/ 35504]\n",
      "loss: 1578.291992  [ 31040/ 35504]\n",
      "loss: 1131.778320  [ 31072/ 35504]\n",
      "loss: 1402.636230  [ 31104/ 35504]\n",
      "loss: 1542.343018  [ 31136/ 35504]\n",
      "loss: 1538.871582  [ 31168/ 35504]\n",
      "loss: 2198.137939  [ 31200/ 35504]\n",
      "loss: 1904.661011  [ 31232/ 35504]\n",
      "loss: 1427.849731  [ 31264/ 35504]\n",
      "loss: 1895.103149  [ 31296/ 35504]\n",
      "loss: 2074.548584  [ 31328/ 35504]\n",
      "loss: 2035.876221  [ 31360/ 35504]\n",
      "loss: 1107.345337  [ 31392/ 35504]\n",
      "loss: 912.424988  [ 31424/ 35504]\n",
      "loss: 2350.597656  [ 31456/ 35504]\n",
      "loss: 1373.419922  [ 31488/ 35504]\n",
      "loss: 1570.239990  [ 31520/ 35504]\n",
      "loss: 2375.666748  [ 31552/ 35504]\n",
      "loss: 1894.545776  [ 31584/ 35504]\n",
      "loss: 1916.267090  [ 31616/ 35504]\n",
      "loss: 1856.752075  [ 31648/ 35504]\n",
      "loss: 1419.309692  [ 31680/ 35504]\n",
      "loss: 2206.595215  [ 31712/ 35504]\n",
      "loss: 1492.087646  [ 31744/ 35504]\n",
      "loss: 1402.386475  [ 31776/ 35504]\n",
      "loss: 2131.010986  [ 31808/ 35504]\n",
      "loss: 1351.352295  [ 31840/ 35504]\n",
      "loss: 1306.296509  [ 31872/ 35504]\n",
      "loss: 2310.496338  [ 31904/ 35504]\n",
      "loss: 2684.952148  [ 31936/ 35504]\n",
      "loss: 1147.786987  [ 31968/ 35504]\n",
      "loss: 1111.338501  [ 32000/ 35504]\n",
      "loss: 1235.049561  [ 32032/ 35504]\n",
      "loss: 2281.950684  [ 32064/ 35504]\n",
      "loss: 2007.190186  [ 32096/ 35504]\n",
      "loss: 1466.605347  [ 32128/ 35504]\n",
      "loss: 2105.399658  [ 32160/ 35504]\n",
      "loss: 1169.799438  [ 32192/ 35504]\n",
      "loss: 1995.753052  [ 32224/ 35504]\n",
      "loss: 1668.799194  [ 32256/ 35504]\n",
      "loss: 1737.796021  [ 32288/ 35504]\n",
      "loss: 1873.937500  [ 32320/ 35504]\n",
      "loss: 2088.886230  [ 32352/ 35504]\n",
      "loss: 848.469604  [ 32384/ 35504]\n",
      "loss: 1761.986084  [ 32416/ 35504]\n",
      "loss: 1333.580200  [ 32448/ 35504]\n",
      "loss: 1386.274170  [ 32480/ 35504]\n",
      "loss: 1128.987183  [ 32512/ 35504]\n",
      "loss: 1604.932373  [ 32544/ 35504]\n",
      "loss: 2480.299316  [ 32576/ 35504]\n",
      "loss: 1974.657715  [ 32608/ 35504]\n",
      "loss: 1900.356934  [ 32640/ 35504]\n",
      "loss: 1368.782227  [ 32672/ 35504]\n",
      "loss: 2394.098633  [ 32704/ 35504]\n",
      "loss: 1495.686279  [ 32736/ 35504]\n",
      "loss: 715.102234  [ 32768/ 35504]\n",
      "loss: 1571.900879  [ 32800/ 35504]\n",
      "loss: 1469.850098  [ 32832/ 35504]\n",
      "loss: 2098.658936  [ 32864/ 35504]\n",
      "loss: 1061.532227  [ 32896/ 35504]\n",
      "loss: 1211.542358  [ 32928/ 35504]\n",
      "loss: 1545.698242  [ 32960/ 35504]\n",
      "loss: 1841.076172  [ 32992/ 35504]\n",
      "loss: 2062.328613  [ 33024/ 35504]\n",
      "loss: 919.936279  [ 33056/ 35504]\n",
      "loss: 2437.249512  [ 33088/ 35504]\n",
      "loss: 1481.376099  [ 33120/ 35504]\n",
      "loss: 1524.592529  [ 33152/ 35504]\n",
      "loss: 1510.072998  [ 33184/ 35504]\n",
      "loss: 578.061279  [ 33216/ 35504]\n",
      "loss: 1715.670898  [ 33248/ 35504]\n",
      "loss: 1562.250122  [ 33280/ 35504]\n",
      "loss: 1922.956665  [ 33312/ 35504]\n",
      "loss: 869.723083  [ 33344/ 35504]\n",
      "loss: 1148.922241  [ 33376/ 35504]\n",
      "loss: 1464.724609  [ 33408/ 35504]\n",
      "loss: 1523.175537  [ 33440/ 35504]\n",
      "loss: 1434.373291  [ 33472/ 35504]\n",
      "loss: 1732.403320  [ 33504/ 35504]\n",
      "loss: 1086.735962  [ 33536/ 35504]\n",
      "loss: 1540.453369  [ 33568/ 35504]\n",
      "loss: 1562.633667  [ 33600/ 35504]\n",
      "loss: 1409.683350  [ 33632/ 35504]\n",
      "loss: 1813.053101  [ 33664/ 35504]\n",
      "loss: 1462.051270  [ 33696/ 35504]\n",
      "loss: 1393.967163  [ 33728/ 35504]\n",
      "loss: 1083.344849  [ 33760/ 35504]\n",
      "loss: 1333.492920  [ 33792/ 35504]\n",
      "loss: 1072.720581  [ 33824/ 35504]\n",
      "loss: 1346.251587  [ 33856/ 35504]\n",
      "loss: 1848.661743  [ 33888/ 35504]\n",
      "loss: 1691.382202  [ 33920/ 35504]\n",
      "loss: 1987.061279  [ 33952/ 35504]\n",
      "loss: 1129.426636  [ 33984/ 35504]\n",
      "loss: 1284.866089  [ 34016/ 35504]\n",
      "loss: 2357.576660  [ 34048/ 35504]\n",
      "loss: 1954.865845  [ 34080/ 35504]\n",
      "loss: 1623.196411  [ 34112/ 35504]\n",
      "loss: 1408.810913  [ 34144/ 35504]\n",
      "loss: 3101.524170  [ 34176/ 35504]\n",
      "loss: 980.857483  [ 34208/ 35504]\n",
      "loss: 939.411987  [ 34240/ 35504]\n",
      "loss: 1389.173706  [ 34272/ 35504]\n",
      "loss: 1232.633911  [ 34304/ 35504]\n",
      "loss: 2032.858887  [ 34336/ 35504]\n",
      "loss: 2297.312744  [ 34368/ 35504]\n",
      "loss: 1354.429321  [ 34400/ 35504]\n",
      "loss: 754.671021  [ 34432/ 35504]\n",
      "loss: 1361.115845  [ 34464/ 35504]\n",
      "loss: 1205.294800  [ 34496/ 35504]\n",
      "loss: 1044.235840  [ 34528/ 35504]\n",
      "loss: 1722.060913  [ 34560/ 35504]\n",
      "loss: 1686.638062  [ 34592/ 35504]\n",
      "loss: 1187.676025  [ 34624/ 35504]\n",
      "loss: 1387.835815  [ 34656/ 35504]\n",
      "loss: 2010.104736  [ 34688/ 35504]\n",
      "loss: 1763.498901  [ 34720/ 35504]\n",
      "loss: 1427.874146  [ 34752/ 35504]\n",
      "loss: 744.709778  [ 34784/ 35504]\n",
      "loss: 786.342285  [ 34816/ 35504]\n",
      "loss: 1653.353638  [ 34848/ 35504]\n",
      "loss: 1123.209961  [ 34880/ 35504]\n",
      "loss: 853.568359  [ 34912/ 35504]\n",
      "loss: 1498.769775  [ 34944/ 35504]\n",
      "loss: 1191.676758  [ 34976/ 35504]\n",
      "loss: 1221.551270  [ 35008/ 35504]\n",
      "loss: 1598.835205  [ 35040/ 35504]\n",
      "loss: 1949.297363  [ 35072/ 35504]\n",
      "loss: 1104.000244  [ 35104/ 35504]\n",
      "loss: 721.189575  [ 35136/ 35504]\n",
      "loss: 1868.221436  [ 35168/ 35504]\n",
      "loss: 1991.530273  [ 35200/ 35504]\n",
      "loss: 1454.688843  [ 35232/ 35504]\n",
      "loss: 1563.314453  [ 35264/ 35504]\n",
      "loss: 1060.752808  [ 35296/ 35504]\n",
      "loss: 922.567383  [ 35328/ 35504]\n",
      "loss: 2088.861084  [ 35360/ 35504]\n",
      "loss: 662.989990  [ 35392/ 35504]\n",
      "loss: 1447.545654  [ 35424/ 35504]\n",
      "loss: 1233.395996  [ 35456/ 35504]\n",
      "loss: 1197.981201  [ 35488/ 35504]\n",
      "loss: 998.802979  [ 35520/ 35504]\n",
      "loss: 917.134399  [ 35552/ 35504]\n",
      "loss: 2058.955811  [ 35584/ 35504]\n",
      "loss: 964.095398  [ 35616/ 35504]\n",
      "loss: 1453.477051  [ 35648/ 35504]\n",
      "loss: 2070.405029  [ 35680/ 35504]\n",
      "loss: 1151.280029  [ 35712/ 35504]\n",
      "loss: 1244.829590  [ 35744/ 35504]\n",
      "loss: 1293.675537  [ 35776/ 35504]\n",
      "loss: 726.535339  [ 35808/ 35504]\n",
      "loss: 1732.890015  [ 35840/ 35504]\n",
      "loss: 1266.127930  [ 35872/ 35504]\n",
      "loss: 879.105957  [ 35904/ 35504]\n",
      "loss: 823.724670  [ 35936/ 35504]\n",
      "loss: 1872.314331  [ 35968/ 35504]\n",
      "loss: 2040.445801  [ 36000/ 35504]\n",
      "loss: 1121.654785  [ 36032/ 35504]\n",
      "loss: 1096.713257  [ 36064/ 35504]\n",
      "loss: 927.056885  [ 36096/ 35504]\n",
      "loss: 1745.622070  [ 36128/ 35504]\n",
      "loss: 852.514282  [ 36160/ 35504]\n",
      "loss: 868.423340  [ 36192/ 35504]\n",
      "loss: 1153.120361  [ 36224/ 35504]\n",
      "loss: 1932.191528  [ 36256/ 35504]\n",
      "loss: 780.859497  [ 36288/ 35504]\n",
      "loss: 646.651245  [ 36320/ 35504]\n",
      "loss: 1351.028076  [ 36352/ 35504]\n",
      "loss: 1305.875000  [ 36384/ 35504]\n",
      "loss: 1131.837646  [ 36416/ 35504]\n",
      "loss: 1369.354004  [ 36448/ 35504]\n",
      "loss: 1148.697632  [ 36480/ 35504]\n",
      "loss: 1392.558716  [ 36512/ 35504]\n",
      "loss: 1284.331299  [ 36544/ 35504]\n",
      "loss: 1666.754883  [ 36576/ 35504]\n",
      "loss: 1774.370605  [ 36608/ 35504]\n",
      "loss: 1494.108398  [ 36640/ 35504]\n",
      "loss: 669.775574  [ 36672/ 35504]\n",
      "loss: 1597.073486  [ 36704/ 35504]\n",
      "loss: 1257.308472  [ 36736/ 35504]\n",
      "loss: 773.108765  [ 36768/ 35504]\n",
      "loss: 695.949036  [ 36800/ 35504]\n",
      "loss: 952.404114  [ 36832/ 35504]\n",
      "loss: 898.687134  [ 36864/ 35504]\n",
      "loss: 1387.186646  [ 36896/ 35504]\n",
      "loss: 1367.993774  [ 36928/ 35504]\n",
      "loss: 1643.613770  [ 36960/ 35504]\n",
      "loss: 1314.802734  [ 36992/ 35504]\n",
      "loss: 2148.149414  [ 37024/ 35504]\n",
      "loss: 946.676147  [ 37056/ 35504]\n",
      "loss: 916.665771  [ 37088/ 35504]\n",
      "loss: 1165.419800  [ 37120/ 35504]\n",
      "loss: 1400.426392  [ 37152/ 35504]\n",
      "loss: 1158.109497  [ 37184/ 35504]\n",
      "loss: 887.977173  [ 37216/ 35504]\n",
      "loss: 1478.489990  [ 37248/ 35504]\n",
      "loss: 896.759583  [ 37280/ 35504]\n",
      "loss: 1269.887817  [ 37312/ 35504]\n",
      "loss: 1356.529663  [ 37344/ 35504]\n",
      "loss: 1756.492676  [ 37376/ 35504]\n",
      "loss: 918.029846  [ 37408/ 35504]\n",
      "loss: 1571.773804  [ 37440/ 35504]\n",
      "loss: 1132.404053  [ 37472/ 35504]\n",
      "loss: 1232.295776  [ 37504/ 35504]\n",
      "loss: 751.687805  [ 37536/ 35504]\n",
      "loss: 544.340942  [ 37568/ 35504]\n",
      "loss: 1003.929382  [ 37600/ 35504]\n",
      "loss: 624.035278  [ 37632/ 35504]\n",
      "loss: 805.465515  [ 37664/ 35504]\n",
      "loss: 1199.722656  [ 37696/ 35504]\n",
      "loss: 1274.776001  [ 37728/ 35504]\n",
      "loss: 1161.642700  [ 37760/ 35504]\n",
      "loss: 1319.291992  [ 37792/ 35504]\n",
      "loss: 1597.584351  [ 37824/ 35504]\n",
      "loss: 904.439636  [ 37856/ 35504]\n",
      "loss: 1345.289795  [ 37888/ 35504]\n",
      "loss: 1212.270386  [ 37920/ 35504]\n",
      "loss: 675.734375  [ 37952/ 35504]\n",
      "loss: 662.294067  [ 37984/ 35504]\n",
      "loss: 1256.858154  [ 38016/ 35504]\n",
      "loss: 1112.393066  [ 38048/ 35504]\n",
      "loss: 1896.978882  [ 38080/ 35504]\n",
      "loss: 1255.138062  [ 38112/ 35504]\n",
      "loss: 930.069824  [ 38144/ 35504]\n",
      "loss: 783.427795  [ 38176/ 35504]\n",
      "loss: 1026.109253  [ 38208/ 35504]\n",
      "loss: 1055.533081  [ 38240/ 35504]\n",
      "loss: 924.437744  [ 38272/ 35504]\n",
      "loss: 959.186523  [ 38304/ 35504]\n",
      "loss: 1285.557251  [ 38336/ 35504]\n",
      "loss: 1136.498657  [ 38368/ 35504]\n",
      "loss: 1161.588867  [ 38400/ 35504]\n",
      "loss: 1129.662720  [ 38432/ 35504]\n",
      "loss: 1132.930542  [ 38464/ 35504]\n",
      "loss: 928.360413  [ 38496/ 35504]\n",
      "loss: 1356.220581  [ 38528/ 35504]\n",
      "loss: 1030.775757  [ 38560/ 35504]\n",
      "loss: 681.256104  [ 38592/ 35504]\n",
      "loss: 747.171204  [ 38624/ 35504]\n",
      "loss: 576.590332  [ 38656/ 35504]\n",
      "loss: 1300.352783  [ 38688/ 35504]\n",
      "loss: 1327.781616  [ 38720/ 35504]\n",
      "loss: 739.815369  [ 38752/ 35504]\n",
      "loss: 1363.497192  [ 38784/ 35504]\n",
      "loss: 1089.857910  [ 38816/ 35504]\n",
      "loss: 1134.659912  [ 38848/ 35504]\n",
      "loss: 1254.254150  [ 38880/ 35504]\n",
      "loss: 975.570129  [ 38912/ 35504]\n",
      "loss: 709.467346  [ 38944/ 35504]\n",
      "loss: 860.645569  [ 38976/ 35504]\n",
      "loss: 731.216003  [ 39008/ 35504]\n",
      "loss: 1134.338013  [ 39040/ 35504]\n",
      "loss: 760.527832  [ 39072/ 35504]\n",
      "loss: 844.850769  [ 39104/ 35504]\n",
      "loss: 1745.868652  [ 39136/ 35504]\n",
      "loss: 1686.054932  [ 39168/ 35504]\n",
      "loss: 1601.507568  [ 39200/ 35504]\n",
      "loss: 918.640015  [ 39232/ 35504]\n",
      "loss: 606.629761  [ 39264/ 35504]\n",
      "loss: 969.411194  [ 39296/ 35504]\n",
      "loss: 864.330139  [ 39328/ 35504]\n",
      "loss: 1254.383667  [ 39360/ 35504]\n",
      "loss: 1057.237305  [ 39392/ 35504]\n",
      "loss: 851.188599  [ 39424/ 35504]\n",
      "loss: 769.630493  [ 39456/ 35504]\n",
      "loss: 859.253052  [ 39488/ 35504]\n",
      "loss: 1855.646118  [ 39520/ 35504]\n",
      "loss: 750.584900  [ 39552/ 35504]\n",
      "loss: 1112.433838  [ 39584/ 35504]\n",
      "loss: 990.873840  [ 39616/ 35504]\n",
      "loss: 562.750366  [ 39648/ 35504]\n",
      "loss: 743.965820  [ 39680/ 35504]\n",
      "loss: 809.549438  [ 39712/ 35504]\n",
      "loss: 916.276917  [ 39744/ 35504]\n",
      "loss: 1214.825928  [ 39776/ 35504]\n",
      "loss: 749.658264  [ 39808/ 35504]\n",
      "loss: 1247.329712  [ 39840/ 35504]\n",
      "loss: 404.405762  [ 39872/ 35504]\n",
      "loss: 1435.355591  [ 39904/ 35504]\n",
      "loss: 745.469788  [ 39936/ 35504]\n",
      "loss: 1329.800903  [ 39968/ 35504]\n",
      "loss: 791.447937  [ 40000/ 35504]\n",
      "loss: 1005.740234  [ 40032/ 35504]\n",
      "loss: 1133.573608  [ 40064/ 35504]\n",
      "loss: 1246.601318  [ 40096/ 35504]\n",
      "loss: 678.994263  [ 40128/ 35504]\n",
      "loss: 759.481567  [ 40160/ 35504]\n",
      "loss: 1044.381104  [ 40192/ 35504]\n",
      "loss: 352.574463  [ 40224/ 35504]\n",
      "loss: 1025.082886  [ 40256/ 35504]\n",
      "loss: 1112.276123  [ 40288/ 35504]\n",
      "loss: 639.200623  [ 40320/ 35504]\n",
      "loss: 1407.704468  [ 40352/ 35504]\n",
      "loss: 1797.532715  [ 40384/ 35504]\n",
      "loss: 926.955994  [ 40416/ 35504]\n",
      "loss: 1225.549805  [ 40448/ 35504]\n",
      "loss: 1252.125366  [ 40480/ 35504]\n",
      "loss: 808.070190  [ 40512/ 35504]\n",
      "loss: 986.198853  [ 40544/ 35504]\n",
      "loss: 691.948853  [ 40576/ 35504]\n",
      "loss: 1342.122803  [ 40608/ 35504]\n",
      "loss: 1089.124390  [ 40640/ 35504]\n",
      "loss: 476.999451  [ 40672/ 35504]\n",
      "loss: 729.124939  [ 40704/ 35504]\n",
      "loss: 888.793335  [ 40736/ 35504]\n",
      "loss: 607.717651  [ 40768/ 35504]\n",
      "loss: 795.959351  [ 40800/ 35504]\n",
      "loss: 893.391602  [ 40832/ 35504]\n",
      "loss: 1029.985352  [ 40864/ 35504]\n",
      "loss: 773.571106  [ 40896/ 35504]\n",
      "loss: 924.259644  [ 40928/ 35504]\n",
      "loss: 909.616211  [ 40960/ 35504]\n",
      "loss: 980.402466  [ 40992/ 35504]\n",
      "loss: 1137.893066  [ 41024/ 35504]\n",
      "loss: 534.356506  [ 41056/ 35504]\n",
      "loss: 1645.299194  [ 41088/ 35504]\n",
      "loss: 862.680908  [ 41120/ 35504]\n",
      "loss: 705.363525  [ 41152/ 35504]\n",
      "loss: 466.621674  [ 41184/ 35504]\n",
      "loss: 511.408813  [ 41216/ 35504]\n",
      "loss: 715.938293  [ 41248/ 35504]\n",
      "loss: 845.166016  [ 41280/ 35504]\n",
      "loss: 809.526367  [ 41312/ 35504]\n",
      "loss: 959.607605  [ 41344/ 35504]\n",
      "loss: 782.206665  [ 41376/ 35504]\n",
      "loss: 1126.627441  [ 41408/ 35504]\n",
      "loss: 472.545410  [ 41440/ 35504]\n",
      "loss: 1268.121216  [ 41472/ 35504]\n",
      "loss: 727.099365  [ 41504/ 35504]\n",
      "loss: 1079.921265  [ 41536/ 35504]\n",
      "loss: 1106.457642  [ 41568/ 35504]\n",
      "loss: 512.750977  [ 41600/ 35504]\n",
      "loss: 967.586548  [ 41632/ 35504]\n",
      "loss: 706.453613  [ 41664/ 35504]\n",
      "loss: 1408.179077  [ 41696/ 35504]\n",
      "loss: 755.325500  [ 41728/ 35504]\n",
      "loss: 682.437866  [ 41760/ 35504]\n",
      "loss: 944.545715  [ 41792/ 35504]\n",
      "loss: 1135.925293  [ 41824/ 35504]\n",
      "loss: 941.682617  [ 41856/ 35504]\n",
      "loss: 517.006958  [ 41888/ 35504]\n",
      "loss: 929.296936  [ 41920/ 35504]\n",
      "loss: 1212.356323  [ 41952/ 35504]\n",
      "loss: 802.358398  [ 41984/ 35504]\n",
      "loss: 769.750427  [ 42016/ 35504]\n",
      "loss: 423.385803  [ 42048/ 35504]\n",
      "loss: 1295.687378  [ 42080/ 35504]\n",
      "loss: 721.144653  [ 42112/ 35504]\n",
      "loss: 958.486633  [ 42144/ 35504]\n",
      "loss: 623.348755  [ 42176/ 35504]\n",
      "loss: 963.517029  [ 42208/ 35504]\n",
      "loss: 629.826233  [ 42240/ 35504]\n",
      "loss: 1248.529419  [ 42272/ 35504]\n",
      "loss: 1232.826660  [ 42304/ 35504]\n",
      "loss: 1334.486572  [ 42336/ 35504]\n",
      "loss: 719.220825  [ 42368/ 35504]\n",
      "loss: 1035.592285  [ 42400/ 35504]\n",
      "loss: 966.302368  [ 42432/ 35504]\n",
      "loss: 1074.234009  [ 42464/ 35504]\n",
      "loss: 792.550720  [ 42496/ 35504]\n",
      "loss: 511.087158  [ 42528/ 35504]\n",
      "loss: 1460.398315  [ 42560/ 35504]\n",
      "loss: 523.812561  [ 42592/ 35504]\n",
      "loss: 1012.283447  [ 42624/ 35504]\n",
      "loss: 503.313782  [ 42656/ 35504]\n",
      "loss: 1238.311035  [ 42688/ 35504]\n",
      "loss: 1502.304077  [ 42720/ 35504]\n",
      "loss: 672.309082  [ 42752/ 35504]\n",
      "loss: 687.536377  [ 42784/ 35504]\n",
      "loss: 563.510071  [ 42816/ 35504]\n",
      "loss: 1961.976440  [ 42848/ 35504]\n",
      "loss: 609.820679  [ 42880/ 35504]\n",
      "loss: 568.129028  [ 42912/ 35504]\n",
      "loss: 804.612549  [ 42944/ 35504]\n",
      "loss: 935.623962  [ 42976/ 35504]\n",
      "loss: 1193.719360  [ 43008/ 35504]\n",
      "loss: 854.876587  [ 43040/ 35504]\n",
      "loss: 1071.875610  [ 43072/ 35504]\n",
      "loss: 1092.570679  [ 43104/ 35504]\n",
      "loss: 1234.049072  [ 43136/ 35504]\n",
      "loss: 809.002136  [ 43168/ 35504]\n",
      "loss: 1273.828857  [ 43200/ 35504]\n",
      "loss: 1163.886841  [ 43232/ 35504]\n",
      "loss: 487.966034  [ 43264/ 35504]\n",
      "loss: 802.455933  [ 43296/ 35504]\n",
      "loss: 444.039001  [ 43328/ 35504]\n",
      "loss: 494.935486  [ 43360/ 35504]\n",
      "loss: 1695.738403  [ 43392/ 35504]\n",
      "loss: 965.831909  [ 43424/ 35504]\n",
      "loss: 936.868286  [ 43456/ 35504]\n",
      "loss: 569.056458  [ 43488/ 35504]\n",
      "loss: 547.533875  [ 43520/ 35504]\n",
      "loss: 884.168213  [ 43552/ 35504]\n",
      "loss: 1110.204834  [ 43584/ 35504]\n",
      "loss: 911.412537  [ 43616/ 35504]\n",
      "loss: 997.018066  [ 43648/ 35504]\n",
      "loss: 315.131195  [ 43680/ 35504]\n",
      "loss: 617.562500  [ 43712/ 35504]\n",
      "loss: 625.036804  [ 43744/ 35504]\n",
      "loss: 329.997772  [ 43776/ 35504]\n",
      "loss: 1218.149902  [ 43808/ 35504]\n",
      "loss: 623.645386  [ 43840/ 35504]\n",
      "loss: 658.587219  [ 43872/ 35504]\n",
      "loss: 736.840210  [ 43904/ 35504]\n",
      "loss: 672.857422  [ 43936/ 35504]\n",
      "loss: 286.468567  [ 43968/ 35504]\n",
      "loss: 1765.841309  [ 44000/ 35504]\n",
      "loss: 1556.039795  [ 44032/ 35504]\n",
      "loss: 724.281494  [ 44064/ 35504]\n",
      "loss: 842.863953  [ 44096/ 35504]\n",
      "loss: 753.370789  [ 44128/ 35504]\n",
      "loss: 534.286194  [ 44160/ 35504]\n",
      "loss: 1067.311035  [ 44192/ 35504]\n",
      "loss: 485.089447  [ 44224/ 35504]\n",
      "loss: 1341.672729  [ 44256/ 35504]\n",
      "loss: 761.644165  [ 44288/ 35504]\n",
      "loss: 760.907959  [ 44320/ 35504]\n",
      "loss: 984.983643  [ 44352/ 35504]\n",
      "loss: 1116.018799  [ 44384/ 35504]\n",
      "loss: 1306.265503  [ 44416/ 35504]\n",
      "loss: 783.718201  [ 44448/ 35504]\n",
      "loss: 584.909058  [ 44480/ 35504]\n",
      "loss: 1335.756348  [ 44512/ 35504]\n",
      "loss: 656.783081  [ 44544/ 35504]\n",
      "loss: 868.341797  [ 44576/ 35504]\n",
      "loss: 1340.904297  [ 44608/ 35504]\n",
      "loss: 659.282532  [ 44640/ 35504]\n",
      "loss: 795.361450  [ 44672/ 35504]\n",
      "loss: 899.826233  [ 44704/ 35504]\n",
      "loss: 211.066650  [ 44736/ 35504]\n",
      "loss: 463.181702  [ 44768/ 35504]\n",
      "loss: 904.492432  [ 44800/ 35504]\n",
      "loss: 1075.015869  [ 44832/ 35504]\n",
      "loss: 542.085144  [ 44864/ 35504]\n",
      "loss: 897.005127  [ 44896/ 35504]\n",
      "loss: 1254.293091  [ 44928/ 35504]\n",
      "loss: 908.573425  [ 44960/ 35504]\n",
      "loss: 820.422119  [ 44992/ 35504]\n",
      "loss: 610.735168  [ 45024/ 35504]\n",
      "loss: 985.607300  [ 45056/ 35504]\n",
      "loss: 527.427856  [ 45088/ 35504]\n",
      "loss: 369.252258  [ 45120/ 35504]\n",
      "loss: 624.568726  [ 45152/ 35504]\n",
      "loss: 716.627258  [ 45184/ 35504]\n",
      "loss: 1109.398438  [ 45216/ 35504]\n",
      "loss: 693.078613  [ 45248/ 35504]\n",
      "loss: 1337.276001  [ 45280/ 35504]\n",
      "loss: 787.861633  [ 45312/ 35504]\n",
      "loss: 835.600586  [ 45344/ 35504]\n",
      "loss: 1362.586914  [ 45376/ 35504]\n",
      "loss: 1076.115479  [ 45408/ 35504]\n",
      "loss: 946.675781  [ 45440/ 35504]\n",
      "loss: 485.995178  [ 45472/ 35504]\n",
      "loss: 632.006409  [ 45504/ 35504]\n",
      "loss: 953.353149  [ 45536/ 35504]\n",
      "loss: 560.814697  [ 45568/ 35504]\n",
      "loss: 1209.534546  [ 45600/ 35504]\n",
      "loss: 971.071838  [ 45632/ 35504]\n",
      "loss: 574.053345  [ 45664/ 35504]\n",
      "loss: 1222.702393  [ 45696/ 35504]\n",
      "loss: 1113.776001  [ 45728/ 35504]\n",
      "loss: 618.102783  [ 45760/ 35504]\n",
      "loss: 676.046509  [ 45792/ 35504]\n",
      "loss: 574.755371  [ 45824/ 35504]\n",
      "loss: 581.116699  [ 45856/ 35504]\n",
      "loss: 369.874207  [ 45888/ 35504]\n",
      "loss: 439.392181  [ 45920/ 35504]\n",
      "loss: 456.483093  [ 45952/ 35504]\n",
      "loss: 622.347656  [ 45984/ 35504]\n",
      "loss: 755.235107  [ 46016/ 35504]\n",
      "loss: 874.867004  [ 46048/ 35504]\n",
      "loss: 882.615906  [ 46080/ 35504]\n",
      "loss: 793.143860  [ 46112/ 35504]\n",
      "loss: 1134.558105  [ 46144/ 35504]\n",
      "loss: 624.534790  [ 46176/ 35504]\n",
      "loss: 1115.226807  [ 46208/ 35504]\n",
      "loss: 253.254929  [ 46240/ 35504]\n",
      "loss: 794.913330  [ 46272/ 35504]\n",
      "loss: 711.927856  [ 46304/ 35504]\n",
      "loss: 581.596802  [ 46336/ 35504]\n",
      "loss: 913.685913  [ 46368/ 35504]\n",
      "loss: 777.869995  [ 46400/ 35504]\n",
      "loss: 512.196045  [ 46432/ 35504]\n",
      "loss: 526.642822  [ 46464/ 35504]\n",
      "loss: 273.387054  [ 46496/ 35504]\n",
      "loss: 984.611694  [ 46528/ 35504]\n",
      "loss: 1100.883057  [ 46560/ 35504]\n",
      "loss: 1256.260742  [ 46592/ 35504]\n",
      "loss: 1136.847656  [ 46624/ 35504]\n",
      "loss: 559.585693  [ 46656/ 35504]\n",
      "loss: 430.689087  [ 46688/ 35504]\n",
      "loss: 633.052490  [ 46720/ 35504]\n",
      "loss: 472.216827  [ 46752/ 35504]\n",
      "loss: 913.709595  [ 46784/ 35504]\n",
      "loss: 1281.416992  [ 46816/ 35504]\n",
      "loss: 807.927979  [ 46848/ 35504]\n",
      "loss: 705.513550  [ 46880/ 35504]\n",
      "loss: 834.447876  [ 46912/ 35504]\n",
      "loss: 870.920898  [ 46944/ 35504]\n",
      "loss: 584.258362  [ 46976/ 35504]\n",
      "loss: 629.147827  [ 47008/ 35504]\n",
      "loss: 583.243530  [ 47040/ 35504]\n",
      "loss: 390.022705  [ 47072/ 35504]\n",
      "loss: 761.160950  [ 47104/ 35504]\n",
      "loss: 827.927795  [ 47136/ 35504]\n",
      "loss: 508.900146  [ 47168/ 35504]\n",
      "loss: 740.843140  [ 47200/ 35504]\n",
      "loss: 429.126129  [ 47232/ 35504]\n",
      "loss: 679.079346  [ 47264/ 35504]\n",
      "loss: 746.419922  [ 47296/ 35504]\n",
      "loss: 1131.421997  [ 47328/ 35504]\n",
      "loss: 684.379272  [ 47360/ 35504]\n",
      "loss: 367.350555  [ 47392/ 35504]\n",
      "loss: 1002.572815  [ 47424/ 35504]\n",
      "loss: 1017.285767  [ 47456/ 35504]\n",
      "loss: 787.751953  [ 47488/ 35504]\n",
      "loss: 1233.674194  [ 47520/ 35504]\n",
      "loss: 487.218719  [ 47552/ 35504]\n",
      "loss: 479.126251  [ 47584/ 35504]\n",
      "loss: 742.471252  [ 47616/ 35504]\n",
      "loss: 365.373566  [ 47648/ 35504]\n",
      "loss: 549.914734  [ 47680/ 35504]\n",
      "loss: 905.118103  [ 47712/ 35504]\n",
      "loss: 419.693237  [ 47744/ 35504]\n",
      "loss: 641.744446  [ 47776/ 35504]\n",
      "loss: 524.173950  [ 47808/ 35504]\n",
      "loss: 649.744202  [ 47840/ 35504]\n",
      "loss: 430.765900  [ 47872/ 35504]\n",
      "loss: 578.562195  [ 47904/ 35504]\n",
      "loss: 906.114990  [ 47936/ 35504]\n",
      "loss: 740.238953  [ 47968/ 35504]\n",
      "loss: 816.897461  [ 48000/ 35504]\n",
      "loss: 722.382751  [ 48032/ 35504]\n",
      "loss: 882.673462  [ 48064/ 35504]\n",
      "loss: 913.601562  [ 48096/ 35504]\n",
      "loss: 576.101746  [ 48128/ 35504]\n",
      "loss: 1009.536377  [ 48160/ 35504]\n",
      "loss: 909.930359  [ 48192/ 35504]\n",
      "loss: 872.770630  [ 48224/ 35504]\n",
      "loss: 482.784576  [ 48256/ 35504]\n",
      "loss: 555.737122  [ 48288/ 35504]\n",
      "loss: 1055.863647  [ 48320/ 35504]\n",
      "loss: 404.436798  [ 48352/ 35504]\n",
      "loss: 716.166260  [ 48384/ 35504]\n",
      "loss: 569.724243  [ 48416/ 35504]\n",
      "loss: 1047.034302  [ 48448/ 35504]\n",
      "loss: 575.191162  [ 48480/ 35504]\n",
      "loss: 739.371094  [ 48512/ 35504]\n",
      "loss: 798.558228  [ 48544/ 35504]\n",
      "loss: 651.238708  [ 48576/ 35504]\n",
      "loss: 695.349243  [ 48608/ 35504]\n",
      "loss: 828.965637  [ 48640/ 35504]\n",
      "loss: 412.988007  [ 48672/ 35504]\n",
      "loss: 556.799011  [ 48704/ 35504]\n",
      "loss: 503.168701  [ 48736/ 35504]\n",
      "loss: 689.232971  [ 48768/ 35504]\n",
      "loss: 605.454712  [ 48800/ 35504]\n",
      "loss: 477.558868  [ 48832/ 35504]\n",
      "loss: 345.298431  [ 48864/ 35504]\n",
      "loss: 251.774170  [ 48896/ 35504]\n",
      "loss: 574.214478  [ 48928/ 35504]\n",
      "loss: 674.157043  [ 48960/ 35504]\n",
      "loss: 346.991302  [ 48992/ 35504]\n",
      "loss: 657.568481  [ 49024/ 35504]\n",
      "loss: 874.872803  [ 49056/ 35504]\n",
      "loss: 905.673706  [ 49088/ 35504]\n",
      "loss: 497.899139  [ 49120/ 35504]\n",
      "loss: 419.706177  [ 49152/ 35504]\n",
      "loss: 320.372589  [ 49184/ 35504]\n",
      "loss: 690.164368  [ 49216/ 35504]\n",
      "loss: 982.992065  [ 49248/ 35504]\n",
      "loss: 625.824646  [ 49280/ 35504]\n",
      "loss: 515.914429  [ 49312/ 35504]\n",
      "loss: 785.250244  [ 49344/ 35504]\n",
      "loss: 882.246094  [ 49376/ 35504]\n",
      "loss: 1093.529419  [ 49408/ 35504]\n",
      "loss: 393.186005  [ 49440/ 35504]\n",
      "loss: 354.954010  [ 49472/ 35504]\n",
      "loss: 328.826782  [ 49504/ 35504]\n",
      "loss: 788.909302  [ 49536/ 35504]\n",
      "loss: 612.276611  [ 49568/ 35504]\n",
      "loss: 265.700775  [ 49600/ 35504]\n",
      "loss: 608.399902  [ 49632/ 35504]\n",
      "loss: 1092.779541  [ 49664/ 35504]\n",
      "loss: 608.272949  [ 49696/ 35504]\n",
      "loss: 355.791992  [ 49728/ 35504]\n",
      "loss: 467.262177  [ 49760/ 35504]\n",
      "loss: 550.495789  [ 49792/ 35504]\n",
      "loss: 620.132812  [ 49824/ 35504]\n",
      "loss: 882.223267  [ 49856/ 35504]\n",
      "loss: 543.894226  [ 49888/ 35504]\n",
      "loss: 975.314819  [ 49920/ 35504]\n",
      "loss: 540.139404  [ 49952/ 35504]\n",
      "loss: 472.801392  [ 49984/ 35504]\n",
      "loss: 303.332458  [ 50016/ 35504]\n",
      "loss: 753.065979  [ 50048/ 35504]\n",
      "loss: 513.847046  [ 50080/ 35504]\n",
      "loss: 324.184570  [ 50112/ 35504]\n",
      "loss: 428.638275  [ 50144/ 35504]\n",
      "loss: 569.289734  [ 50176/ 35504]\n",
      "loss: 864.711426  [ 50208/ 35504]\n",
      "loss: 503.227081  [ 50240/ 35504]\n",
      "loss: 674.319275  [ 50272/ 35504]\n",
      "loss: 508.950897  [ 50304/ 35504]\n",
      "loss: 411.370422  [ 50336/ 35504]\n",
      "loss: 654.249084  [ 50368/ 35504]\n",
      "loss: 1007.649353  [ 50400/ 35504]\n",
      "loss: 560.570801  [ 50432/ 35504]\n",
      "loss: 681.518799  [ 50464/ 35504]\n",
      "loss: 442.825867  [ 50496/ 35504]\n",
      "loss: 977.450439  [ 50528/ 35504]\n",
      "loss: 613.595154  [ 50560/ 35504]\n",
      "loss: 1047.327393  [ 50592/ 35504]\n",
      "loss: 646.913391  [ 50624/ 35504]\n",
      "loss: 674.292236  [ 50656/ 35504]\n",
      "loss: 267.280457  [ 50688/ 35504]\n",
      "loss: 790.838135  [ 50720/ 35504]\n",
      "loss: 649.295410  [ 50752/ 35504]\n",
      "loss: 414.758240  [ 50784/ 35504]\n",
      "loss: 676.611206  [ 50816/ 35504]\n",
      "loss: 584.327393  [ 50848/ 35504]\n",
      "loss: 260.848907  [ 50880/ 35504]\n",
      "loss: 849.501160  [ 50912/ 35504]\n",
      "loss: 629.599243  [ 50944/ 35504]\n",
      "loss: 434.726532  [ 50976/ 35504]\n",
      "loss: 352.378937  [ 51008/ 35504]\n",
      "loss: 756.720459  [ 51040/ 35504]\n",
      "loss: 847.018066  [ 51072/ 35504]\n",
      "loss: 961.656616  [ 51104/ 35504]\n",
      "loss: 517.783936  [ 51136/ 35504]\n",
      "loss: 692.512329  [ 51168/ 35504]\n",
      "loss: 535.198120  [ 51200/ 35504]\n",
      "loss: 506.861023  [ 51232/ 35504]\n",
      "loss: 267.891937  [ 51264/ 35504]\n",
      "loss: 384.421478  [ 51296/ 35504]\n",
      "loss: 358.811371  [ 51328/ 35504]\n",
      "loss: 895.207458  [ 51360/ 35504]\n",
      "loss: 479.902618  [ 51392/ 35504]\n",
      "loss: 648.978333  [ 51424/ 35504]\n",
      "loss: 532.534546  [ 51456/ 35504]\n",
      "loss: 561.364746  [ 51488/ 35504]\n",
      "loss: 624.749878  [ 51520/ 35504]\n",
      "loss: 389.616821  [ 51552/ 35504]\n",
      "loss: 669.104248  [ 51584/ 35504]\n",
      "loss: 669.352112  [ 51616/ 35504]\n",
      "loss: 907.752991  [ 51648/ 35504]\n",
      "loss: 458.713257  [ 51680/ 35504]\n",
      "loss: 367.053955  [ 51712/ 35504]\n",
      "loss: 601.310913  [ 51744/ 35504]\n",
      "loss: 535.725464  [ 51776/ 35504]\n",
      "loss: 419.312775  [ 51808/ 35504]\n",
      "loss: 544.103027  [ 51840/ 35504]\n",
      "loss: 385.794800  [ 51872/ 35504]\n",
      "loss: 370.789764  [ 51904/ 35504]\n",
      "loss: 754.116638  [ 51936/ 35504]\n",
      "loss: 478.304565  [ 51968/ 35504]\n",
      "loss: 802.786804  [ 52000/ 35504]\n",
      "loss: 495.441376  [ 52032/ 35504]\n",
      "loss: 505.005554  [ 52064/ 35504]\n",
      "loss: 766.000427  [ 52096/ 35504]\n",
      "loss: 790.335938  [ 52128/ 35504]\n",
      "loss: 346.600861  [ 52160/ 35504]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/7t/g74n2nn91bj8jx78qk2_mk7r0000gn/T/ipykernel_13637/136644676.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mmodel_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'./tmp_model/'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mcur_time\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'-'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mmodel_name\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTMPDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me_tmp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0mtrain_ls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_ls\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlog_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.01\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/wpf_contest/notebooks/tmp_train.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(log_dir, model_path, dataset, num_epochs, learning_rate, weight_decay, batch_size, sample_size)\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0msummary\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m288\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol_names\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"input_size\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"kernel_size\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"output_size\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 64\u001B[0;31m     \u001B[0mtrain_ls\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_epochs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_dataloader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnet\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwriter\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     65\u001B[0m     \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_path\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'.pt'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mtrain_ls\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/wpf_contest/notebooks/tmp_train.py\u001B[0m in \u001B[0;36mtrain_epochs\u001B[0;34m(train_dataloader, net, loss_fn, optimizer, writer, num_epochs, batch_size, sample_size)\u001B[0m\n\u001B[1;32m     36\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Epoch {epoch + 1}\\n-------------------------------\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m         \u001B[0mtrain_ls_epoch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_epoch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_dataloader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnet\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m         \u001B[0mtrain_ls_avg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_ls_epoch\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Epoch {epoch + 1}: train loss: {train_ls_avg}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/wpf_contest/notebooks/tmp_train.py\u001B[0m in \u001B[0;36mtrain_epoch\u001B[0;34m(dataloader, net, loss_fn, optimizer, sample_size)\u001B[0m\n\u001B[1;32m     17\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataloader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m         \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m         \u001B[0mpred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     20\u001B[0m         \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpred\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/ml/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/wpf_contest/notebooks/tmp_lstm.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m         \u001B[0mdec\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlstm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m         \u001B[0mpred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprojection\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdec\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/ml/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/ml/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input, hx)\u001B[0m\n\u001B[1;32m    689\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcheck_forward_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_sizes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    690\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mbatch_sizes\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 691\u001B[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001B[0m\u001B[1;32m    692\u001B[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001B[1;32m    693\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model_name = 'tmp_lstm'\n",
    "cur_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"./logs/\" + cur_time + '-' + model_name\n",
    "model_path = './tmp_model/' + cur_time + '-' + model_name\n",
    "dataset = TMPDataset(e_tmp)\n",
    "train_ls = train(log_dir, model_path, dataset, batch_size=batch_size, sample_size=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}